---
title: "model-visualization"
format: html
editor: visual
---

Built using r version 4.3.1.

R was chosen for this analysis because of its integration with the STAN probabilistic programming language and suite of tools used to visualize uncertainty and Bayesian model outputs. 

```{r}
# install.packages(c("tidyverse", "ggthemes", "patchwork", "brms", "ggdist", "tidybayes", "modelr", "arrow", "scales", "sessioninfo"))
```


```{r}
library(tidyverse)
library(ggthemes)
library(patchwork)
library(brms)
library(ggdist)
library(tidybayes)
library(modelr)
```

Defining a style sheet to adhere to styles.css

```{r}
library(ggthemes)

# Define a custom theme
theme_report <- function() {
  theme_minimal() +
  theme(
    text = element_text(family = "Arial", color = "#333333"),
    plot.title = element_text(face = "bold", size = 16, color = "#2c3e50"),
    axis.title = element_text(size = 12, color = "#2c3e50"),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10)
  )
}

# Define a color palette
report_colors <- c("#3498db", "#e74c3c", "#2ecc71", "#f39c12", "#9b59b6", 
                   "#1abc9c", "#d35400", "#34495e", "#7f8c8d", "#2980b9", "#8e44ad")
```

Reading in the city-aggregated dataset imputing 2019 patient experience star ratings and hospital readmission categories values for 2018. 

```{r}
city_df <- read_parquet("../../003_data/003_merged-data/merged_cms_ballot-measures_by-city.parquet")

# Impute 2019 values for patient experience star ratings and hospital readmission categories

city_df <- city_df  |> 
  # First,we create a temporary dataframe with 2019 values for variables we need to impute
  left_join(
    city_df |> 
      filter(year == 2019) |>
      select(
        provider_number, 
        ich_cahps_survey_of_patients_experiences_star_rating_2019 = ich_cahps_survey_of_patients_experiences_star_rating,
        patient_hospital_readmission_category_2019 = patient_hospital_readmission_category
      ),
    by = "provider_number"
  ) |>
  # Then, we replace 2018 values with 2019 values for both variables
  mutate(
    ich_cahps_survey_of_patients_experiences_star_rating = case_when(
      year == 2018 ~ ich_cahps_survey_of_patients_experiences_star_rating_2019,
      TRUE ~ ich_cahps_survey_of_patients_experiences_star_rating
    ),
    patient_hospital_readmission_category = case_when(
      year == 2018 ~ patient_hospital_readmission_category_2019,
      TRUE ~ patient_hospital_readmission_category
    )
  ) |>
  # Finally, we remove the temporary column
  select(-ends_with("_2019"))

```


This below code prepares the dialysis facility data for analysis:

1. Converts quality metrics to numeric type (mortality rate, staff rating, etc.)
2. Creates an ordered factor for hospital readmission categories
3. Filters data for years 2018, 2020, and 2022, removing rows with missing city data
4. Aggregates data by facility and year, calculating:
   - Total votes and yes votes
   - Vote percentage
   - Quality metrics (using first occurrence in group)
5. Selects relevant columns and removes duplicates and rows with missing values

The resulting `filtered_city_df` contains clean, aggregated data for each facility-year combination, including voting results and quality metrics.\

```{r}
filtered_city_df <- (
  city_df 
  |> mutate(mortality_rate_facility = as.numeric(mortality_rate_facility),
            n_dialysis_stations = as.numeric(`_of_dialysis_stations`),
            staff_rating = as.numeric(linearized_score_of_rating_of_the_dialysis_center_staff),
            five_star = as.numeric(five_star),
            patient_experience_rating = as.numeric(ich_cahps_survey_of_patients_experiences_star_rating))
  |> mutate(hospital_readmission = factor(patient_hospital_readmission_category,
                              levels = c("Worse than Expected",
                                         "As Expected",
                                         "Better than Expected",
                                         "Not Available"),
                              ordered = TRUE))
  |> filter(!is.na(city), year %in% c(2022, 2020, 2018))
  |> group_by(year, provider_number, county, city, profit_or_nonprofit, std_chain_organization)
  |> summarize(
    total_votes = sum(vote_count),
    yes_votes = sum(vote_count[vote_type == "yes"]),
    vote_perc = yes_votes / total_votes,
    five_star = first(five_star),
    mortality_rate_facility = first(mortality_rate_facility),
    staff_rating = first(staff_rating),
    patient_experience_rating = first(patient_experience_rating),
    n_dialysis_stations = first(n_dialysis_stations),
    hospital_readmission = first(hospital_readmission)
  )
  |> ungroup()
  |> select(year, provider_number, profit_or_nonprofit, std_chain_organization, county, city, five_star, mortality_rate_facility, staff_rating, patient_experience_rating, n_dialysis_stations, hospital_readmission, vote_perc)
  |> distinct()
  |> drop_na()
)
```

This section defines the model formula and fits the Bayesian regression model using brms.
The model predicts vote percentage based on various variables of interest, including quality metrics (staff rating, mortality rate, patient experience rating), facility characteristics (number of dialysis stations), and other factors (hospital readmission category).

It's commented out because the model has already been fitted and saved as an RDS file, which is loaded in the next code chunk for further analysis.

```{r}
# model_formula <- bf(
#   formula = vote_perc ~ five_star + 
#                        patient_experience_rating +
#                        mortality_rate_facility + 
#                        n_dialysis_stations + 
#                        staff_rating +
#                        mo(hospital_readmission) +
#                        (1 | county) + 
#                        (1 | city) +
#                        (1 | profit_or_nonprofit) +
#                        (1 | std_chain_organization)
# )
# 
# model <- brm(
#   formula = model_formula,
#   family = Beta(),
#   data = filtered_city_df,
#   cores = 4,
#   chains = 4,
#   iter = 2000,
#   warmup = 1000,
#   file = "../006_models/dialysis-model_18"
# )
```

```{r}
model = readRDS("dialysis-model_18.rds")
```

The first plot below shows how the effect of staff rating varies across counties, as well as the uncertainty in each county's effect represented by the width of a given distribution.

Whether each county's effect is significantly different from zero (outside the 80% credible interval, demarcated by the horizontal dashed lines). This plot helps identify counties where staff rating has a particularly strong or weak effect on the vote percentage, which useful for understanding regional variations in the impact of staff ratings on voting behavior.

The second plot, which visualizes intercepts output by the model, helps identify counties that have particularly high or low baseline vote percentages, after accounting for other factors in the model. It's useful for understanding regional variations in voting behavior that aren't explained by the other predictors in the model. For example, San Francisco, Humboldt, Alameda
Contra Costa, Imperial, Monterey, Santa Clara, and Santa Cruz (which are more urban counties) were among the counties with the highest baseline vote percentages, while Shasta, Tehama, and Kings (more rural counties) were among the counties with the lowest baseline vote percentages. 

```{r}
#| label: fig-staff-rating-plot
#| fig-cap: "County-Level Staff Rating Effects"

# Function to format county names
format_county_name <- function(name) {
  name |> 
    str_replace_all("\\.", " ") |>   # Replace dots with spaces
    str_to_title() |>               # Capitalize first letter of each word
    str_replace(" County", "")       # Remove " County" if present
}

# Function to create the plot with improved legend and y-axis labels
create_county_effect_plot <- function(model, effect_type, title, subtitle) {
  model |>
    spread_draws(!!sym(effect_type), r_county[county,]) |>
    mutate(
      county_mean = !!sym(effect_type) + r_county,
      county = format_county_name(county)  # Format county names
    ) |>
    ggplot(aes(y = county, x = county_mean, fill = after_stat(abs(x) < 0.8))) +
    stat_halfeye() +
    geom_vline(xintercept = c(-0.8, 0.8), linetype = "dashed", color = "#2c3e50", alpha = 0.7) +
    scale_fill_manual(
      values = c(report_colors[2], report_colors[1]),
      name = "Effect Magnitude",
      labels = c("Large", "Small")
    ) +
    labs(title = title,
         subtitle = subtitle,
         x = "County Mean",
         y = "County") +
    theme_report() +
    theme(
      axis.text.y = element_text(size = 8, hjust = 1),
      axis.ticks.y = element_blank(),  # Remove y-axis ticks
      panel.grid.major.y = element_line(color = "gray90", size = 0.1),  # Add light horizontal lines
      plot.margin = margin(r = 20),  # Increase right margin for county names
      legend.position = "bottom",
      legend.box = "horizontal",
      legend.margin = margin(t = 10, r = 0, b = 0, l = 0),
      legend.title = element_text(size = 10),
      legend.text = element_text(size = 8)
    ) +
    scale_y_discrete(expand = expansion(add = c(0.5, 1)))  # Increase spacing between county names
}

# Create plot for Staff Rating Effects
staff_rating_plot <- create_county_effect_plot(
  model,
  "b_staff_rating",
  "County-Level Staff Rating Effects",
  "Posterior distribution of county-specific staff rating effects"
)

# Create plot for Intercepts
intercept_plot <- create_county_effect_plot(
  model,
  "b_Intercept",
  "County-Level Intercepts",
  "Posterior distribution of county-specific effects"
)
```

```{r}
#| label: fig-staff-rating-plot
#| fig-cap: "County-Level Staff Rating Effects"
staff_rating_plot
```


```{r}
#| label: fig-intercept-plot
#| fig-cap: "County-Level Intercepts"
intercept_plot
```

The plot below shows a posterior predictive check for the percentage of votes in favor of regulation across different standard chain organizations. This visualization compares the observed data (represented by red diamonds) with simulated data from the model (represented by boxplots).

The boxplots display the distribution of simulated vote percentages for each standard chain organization, with the box representing the interquartile range (IQR) and the whiskers extending to 1.5 times the IQR. This allows us to assess how well the model captures the variability in voting patterns across different organizations.

The red diamonds indicate the observed mean vote percentage for each organization, enabling a direct comparison between the model's predictions and the actual data. If the observed values fall within or close to the boxplots, it suggests that the model is performing well in capturing the voting patterns for that organization.

This plot is particularly useful for identifying organizations where the model's predictions align well with observed data, as well as those where there might be discrepancies. For instance, we might observe that some organizations consistently have higher or lower vote percentages in favor of regulation, which could indicate differences in organizational culture, patient demographics, or other factors not explicitly included in the model.

By comparing the positions of the boxplots relative to each other, we can also gain insights into how different standard chain organizations compare in terms of their tendency to receive votes in favor of regulation. Organizations with higher-positioned boxplots tend to receive more favorable votes, while those with lower-positioned boxplots tend to receive fewer favorable votes.
This visualization helps in understanding the model's performance across different organizations and can highlight areas where the model might need improvement or where further investigation into organization-specific factors might be warranted.

```{r}
pp_group <- function(data, pp_samples, group_var) {
  group_var <- enquo(group_var)
  
  observed <- data |>
    group_by(!!group_var) |>
    summarise(vote_perc = mean(vote_perc))
  
  simulated <- apply(pp_samples, 1, function(x) {
    data |>
      mutate(vote_perc = x) |>
      group_by(!!group_var) |>
      summarise(vote_perc = mean(vote_perc))
  }) |> 
    bind_rows(.id = "simulation")
  
  group_name <- quo_name(group_var)
  
  # Format group names for better readability
  format_group_name <- function(name) {
    name |>
      str_replace_all("_", " ") |>
      str_to_title()
  }
  
  ggplot() +
    # Use coord_flip() to make horizontal boxplots
    coord_flip() +
    geom_boxplot(data = simulated, 
                 aes(x = factor(!!group_var, levels = rev(levels(factor(!!group_var)))), 
                     y = vote_perc, 
                     fill = !!group_var), 
                 alpha = 0.7,
                 width = 0.5) +  # Reduce width of boxplots
    geom_point(data = observed, 
               aes(x = !!group_var, y = vote_perc), 
               color = "#e74c3c", 
               size = 3,
               shape = 18) +  # Use diamond shape for observed points
    theme_report() +
    theme(
      axis.text.y = element_text(size = 10),
      axis.title.y = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank(),
      legend.position = "none",
      plot.caption.position = "plot",
      plot.caption = element_text(hjust = 0.7)
    ) +
    labs(
      title = paste("Posterior Predictive Check by Chain Organization"),
      subtitle = "Percentage of Votes in Favor of Regulation",
      x = NULL,  # Remove x-axis label
      y = "Vote Percentage",
      caption = "Red diamonds represent observed mean values"
    ) +
    scale_y_continuous(
      labels = scales::percent_format(accuracy = 1),
      limits = c(0, 1),  # Set y-axis limits from 0% to 100%
      expand = expansion(mult = c(0, 0.05))
    ) +
    scale_fill_manual(values = report_colors) +
    # Format y-axis labels
    scale_x_discrete(labels = function(x) format_group_name(x))
}

```

```{r}
#| label: fig-pp-group
#| fig-cap: "Posterior Predictive Check for Chain Organizations"
pp_group_plot <- pp_group(filtered_city_df, pp_samples, std_chain_organization)

ggsave("../../007_visualizations/pp_chain_org.png", plot = pp_group_plot, width = 10, height = 6, dpi = 300)

```

This plot visualizes the effect of staff rating, facility mortality rate, and patient experience rating on the predicted vote percentage in favor of regulation, while holding other variables constant at their mean or most common values. It combines observed data points with model predictions to provide a comprehensive view of the relationship between staff ratings and voting outcomes.

The x-axis represents the staff rating, ranging from the minimum to the maximum observed values in the dataset. The y-axis shows the predicted vote percentage, formatted as a percentage for easy interpretation. The light grey dots scattered across the plot represent the actual observed data points, with each dot corresponding to a facility's staff rating and its associated vote percentage.

The central line represents the model's predicted relationship between a given variable and vote percentage when marginalized over all the other variables in the model (i.e. 'controlling' for the other variables). The shaded areas around this line represent the uncertainty in these predictions, with three levels of credible intervals:
1. The widest band (lightest shade) represents the 95% credible interval.
2. The middle band represents the 80% credible interval.
3. The narrowest band (darkest shade) represents the 50% credible interval.

These credible intervals provide a visual representation of the model's certainty about its predictions. Where the bands are narrower, the model is more certain about its predictions; where they are wider, there is more uncertainty.

Staff Rating: 
The overall trend is that as staff rating increases, the predicted vote percentage in favor of regulation decreases. This suggests that in cities and counties where facilities have lower staff ratings, voters are more likely to vote in favor of regulating dialysis clinics.

Mortality Rate: 
The overall trend is that as mortality rate increases, the predicted vote percentage in favor of regulation increases This suggests that in cities and counties where facilities have higher mortality rates, voters are more likely to vote in favor of regulating dialysis clinics.

Patient Experience Rating:
The overall trend is that as patient experience rating increases, the predicted vote percentage in favor of regulation increases This suggests that in cities and counties where facilities have higher patient experience ratings, voters are more likely to vote in favor of regulating dialysis clinics.



```{r}
# Create a new data frame with a range of staff ratings
new_data <- filtered_city_df |>
  data_grid(
    staff_rating = seq(from = min(filtered_city_df$staff_rating),
                       to = max(filtered_city_df$staff_rating),
                       length.out = 100),
    .model = model
  ) |>
  # Set other variables to their mean or most common value
  mutate(
    five_star = mean(filtered_city_df$five_star),
    patient_experience_rating = mean(filtered_city_df$patient_experience_rating),
    mortality_rate_facility = mean(filtered_city_df$mortality_rate_facility),
    n_dialysis_stations = mean(filtered_city_df$n_dialysis_stations),
    hospital_readmission = names(which.max(table(filtered_city_df$hospital_readmission))),
    county = names(which.max(table(filtered_city_df$county))),
    city = names(which.max(table(filtered_city_df$city))),
    profit_or_nonprofit = names(which.max(table(filtered_city_df$profit_or_nonprofit))),
    std_chain_organization = names(which.max(table(filtered_city_df$std_chain_organization)))
  )

# Add predicted draws
predictions <- new_data |>
  add_epred_draws(model, ndraws = 1000)
```

```{r}
#| label: fig-staff-rating-effect
#| fig-cap: "Effect of Staff Rating on Predicted Vote Percentage"
staff_rating_plot <- ggplot(predictions, aes(x = staff_rating, y = .epred)) +
  geom_jitter(data = filtered_city_df, aes(y = vote_perc), alpha = 0.2, color = report_colors[9]) +
  stat_lineribbon(aes(y = .epred), .width = c(.95, .80, .50), alpha = 0.5) +
  labs(
    title = "Effect of Staff Rating on Predicted Vote Percentage",
    subtitle = "Controlling for other variables at their mean or most common values",
    x = "Staff Rating",
    y = "Predicted Vote Percentage",
    caption = "Shaded areas represent 95%, 80%, and 50% credible intervals\nGrey dots represent observed data"
  ) +
  scale_x_continuous(limits = c(75, NA), expand = c(0, 0)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_report() +
  scale_fill_manual(values = report_colors[1:3]) +
  theme(
    legend.position = "none",
    plot.caption = element_text(hjust = 0, size = 8, color = "#7f8c8d"),
    plot.subtitle = element_text(size = 10, color = "#7f8c8d")
  )

ggsave("../../007_visualizations/staff_rating_effect.png", plot = staff_rating_plot, width = 10, height = 6, dpi = 300)

```



```{r}
new_data <- filtered_city_df |>
  data_grid(
    mortality_rate_facility = seq(from = min(filtered_city_df$mortality_rate_facility),
                       to = max(filtered_city_df$mortality_rate_facility),
                       length.out = 100),
    .model = model
  ) |>
  # Set other variables to their mean or most common value
  mutate(
    five_star = mean(filtered_city_df$five_star),
    patient_experience_rating = mean(filtered_city_df$patient_experience_rating),
    staff_rating = mean(filtered_city_df$staff_rating),
    n_dialysis_stations = mean(filtered_city_df$n_dialysis_stations),
    hospital_readmission = names(which.max(table(filtered_city_df$hospital_readmission))),
    county = names(which.max(table(filtered_city_df$county))),
    city = names(which.max(table(filtered_city_df$city))),
    profit_or_nonprofit = names(which.max(table(filtered_city_df$profit_or_nonprofit))),
    std_chain_organization = names(which.max(table(filtered_city_df$std_chain_organization)))
  )

# Adding predicted draws
predictions <- new_data |>
  add_epred_draws(model, ndraws = 1000)
```

```{r}
#| label: fig-mortality-rate-effect
#| fig-cap: "Effect of Mortality Rate on Predicted Vote Percentage"
mortality_rate_plot <- ggplot(predictions, aes(x = mortality_rate_facility, y = .epred)) +
  geom_jitter(data = filtered_city_df, aes(y = vote_perc), alpha = 0.2, color = report_colors[9]) +
  stat_lineribbon(aes(y = .epred), .width = c(.95, .80, .50), alpha = 0.5) +
  labs(
    title = "Effect of Mortality Rate on Predicted Vote Percentage",
    subtitle = "Controlling for other variables at their mean or most common values",
    x = "Mortality Rate",
    y = "Predicted Vote Percentage",
    caption = "Shaded areas represent 95%, 80%, and 50% credible intervals\nGrey dots represent observed data"
  ) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_fill_manual(values = report_colors[1:3]) +
  theme_report() +
  theme(
    legend.position = "none",
    plot.caption = element_text(hjust = 0, size = 8, color = "#7f8c8d"),
    plot.subtitle = element_text(size = 10, color = "#7f8c8d")
  )

ggsave("../../007_visualizations/mortality_rate_effect.png", plot = mortality_rate_plot, width = 10, height = 6, dpi = 300)

```

```{r}
new_data <- filtered_city_df |>
  data_grid(
    patient_experience_rating = seq(from = min(filtered_city_df$patient_experience_rating),
                       to = max(filtered_city_df$patient_experience_rating),
                       length.out = 100),
    .model = model
  ) |>
  # Set other variables to their mean or most common value
  mutate(
    five_star = mean(filtered_city_df$five_star),
    mortality_rate_facility = mean(filtered_city_df$mortality_rate_facility),
    staff_rating = mean(filtered_city_df$staff_rating),
    n_dialysis_stations = mean(filtered_city_df$n_dialysis_stations),
    hospital_readmission = names(which.max(table(filtered_city_df$hospital_readmission))),
    county = names(which.max(table(filtered_city_df$county))),
    city = names(which.max(table(filtered_city_df$city))),
    profit_or_nonprofit = names(which.max(table(filtered_city_df$profit_or_nonprofit))),
    std_chain_organization = names(which.max(table(filtered_city_df$std_chain_organization)))
  )

# Adding predicted draws
predictions <- new_data |>
  add_epred_draws(model, ndraws = 1000)
```

```{r}
#| label: fig-patient-experience-effect
#| fig-cap: "Effect of Patient Experience Rating on Predicted Vote Percentage"
experience_plot <- ggplot(predictions, aes(x = patient_experience_rating, y = .epred)) +
  geom_jitter(data = filtered_city_df, aes(y = vote_perc), alpha = 0.2, color = report_colors[9]) +
  stat_lineribbon(aes(y = .epred), .width = c(.95, .80, .50), alpha = 0.5) +
  labs(
    title = "Effect of Patient Experience Rating on Predicted Vote Percentage",
    subtitle = "Controlling for other variables at their mean or most common values",
    x = "Patient Experience Rating",
    y = "Predicted Vote Percentage",
    caption = "Shaded areas represent 95%, 80%, and 50% credible intervals\nGrey dots represent observed data"
  ) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_fill_manual(values = report_colors[1:3]) +
  theme_report() +
  theme(
    legend.position = "none",
    plot.caption = element_text(hjust = 0, size = 8, color = "#7f8c8d"),
    plot.subtitle = element_text(size = 10, color = "#7f8c8d")
  )

ggsave("../../007_visualizations/patient_experience_effect.png", plot = experience_plot, width = 10, height = 6, dpi = 300)

```

This plot visualizes the posterior distributions of the model parameters, providing insights into the effects of various factors on the predicted vote percentage in favor of regulation. The visualization uses a half-eye plot (also known as a ridgeline plot) to display the distribution of each parameter's posterior samples.

Each row in the plot represents a different model parameter, with the parameter names listed on the y-axis. The x-axis shows the parameter estimates, indicating the magnitude and direction of each parameter's effect on the vote percentage.
The half-eye plot for each parameter consists of three main components:
1. The shaded area represents the full distribution of posterior samples, with darker regions indicating higher density of samples.
2. The thick horizontal bar in the center of each distribution shows the 50% credible interval, which is the range within which we expect the true parameter value to fall with 50% probability.
3. The thin horizontal line extending from the thick bar represents the 95% credible interval, providing a wider range of plausible parameter values.

A vertical dashed line at x=0 is included to easily identify whether a parameter's effect is likely to be positive, negative, or centered around zero (indicating little to no effect).

The parameters are ordered based on their median effect size, with those having the largest positive effect at the top and those with the largest negative effect at the bottom.

```{r}
posterior_samples <- model |>
  spread_draws(b_staff_rating, b_mortality_rate_facility, b_five_star, b_patient_experience_rating, b_n_dialysis_stations) |>
  pivot_longer(cols = starts_with("b_"), names_to = "parameter", values_to = "value") |>
  mutate(
    parameter = str_remove(parameter, "b_"),
    parameter = case_when(
      parameter == "staff_rating" ~ "Staff Rating",
      parameter == "mortality_rate_facility" ~ "Mortality Rate",
      parameter == "five_star" ~ "Five Star Rating",
      parameter == "patient_experience_rating" ~ "Patient Experience",
      parameter == "n_dialysis_stations" ~ "Number of Dialysis Stations",
      TRUE ~ parameter
    ),
    parameter = fct_reorder(parameter, value)
  )

```

```{r}
#| label: fig-posterior-distributions
#| fig-cap: "Posterior Distributions of Model Parameters"
posterior_effects_plot <- ggplot(posterior_samples, aes(y = parameter, x = value, fill = parameter)) +
  stat_halfeye(
    point_interval = "median_qi",
    .width = c(0.5, 0.8, 0.95),
    interval_color = "black",
    interval_alpha = 0.5,
    slab_alpha = 0.5
  ) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "#2c3e50", alpha = 0.7) +
  scale_fill_manual(values = report_colors[1:5]) +
  labs(
    title = "Posterior Distributions of Model Parameters",
    subtitle = "Median and credible intervals shown",
    x = "Parameter Estimate",
    y = NULL,
    caption = "Thick bar: 50% CI | Thin bar: 95% CI | Dashed line: No effect"
  ) +
  theme_report() +
  theme(
    legend.position = "none",
    axis.text.y = element_text(hjust = 1),
    plot.caption = element_text(hjust = 0, size = 8, color = "#7f8c8d"),
    plot.subtitle = element_text(size = 10, color = "#7f8c8d")
  ) +
  scale_x_continuous(
    labels = scales::label_number(accuracy = 0.01),
    breaks = scales::pretty_breaks(n = 6)
  )

ggsave("../../007_visualizations/posterior_effects.png", plot = posterior_effects_plot, width = 10, height = 6, dpi = 300)

```



```{r}
#| label: fig-mcmc-trace
#| fig-cap: "MCMC Trace Plots"
mcmc_trace(model, pars = c("b_mortality_rate_facility", "b_n_dialysis_stations", 
                           "b_five_star", "b_patient_experience_rating", "b_staff_rating")) +
  scale_color_manual(values = report_colors[1:5]) +
  labs(
    title = "MCMC Trace Plots",
    subtitle = "Convergence check for key parameters"
  ) +
  theme_report() +
  theme(legend.position = "none")

```

```{r}
#| label: fig-posterior-density
#| fig-cap: "Posterior Density Plots"
mcmc_dens_overlay(model, pars = c("b_mortality_rate_facility", "b_n_dialysis_stations", 
                                  "b_five_star", "b_patient_experience_rating", "b_staff_rating")) +
  scale_color_manual(values = report_colors[1:5]) +
  labs(
    title = "Posterior Density Plots",
    subtitle = "Overlaid densities for key parameters"
  ) +
  theme_report() +
  theme(legend.position = "bottom")

# Posterior predictive intervals
post_pred <- fitted(model, probs = c(0.025, 0.975))
pred_plot <- ggplot(cbind(filtered_city_df, post_pred), 
                    aes(x = vote_perc, y = Estimate)) +
  geom_abline(intercept = 0, slope = 1, color = report_colors[2], linetype = "dashed") +
  geom_point(color = report_colors[1], alpha = 0.5) +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3, color = report_colors[3]) +
  theme_report() +
  labs(
    title = "Posterior Predictive Intervals",
    subtitle = "Comparing observed vs. predicted vote percentages",
    x = "Observed Vote Percentage",
    y = "Predicted Vote Percentage"
  ) +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1))

```

```{r}
#| label: fig-pred-plot
#| fig-cap: "Posterior Predictive Intervals"
pred_plot
```

```{r}
#| label: fig-pp-check
#| fig-cap: "Posterior Predictive Check"
pp_check_plot <- pp_check(model, type = "dens_overlay", nsamples = 100) +
  scale_color_manual(values = c("black", rep(report_colors[1], 100))) +
  labs(
    title = "Posterior Predictive Check",
    subtitle = "Density overlay of observed vs. simulated data",
    x = "Vote Percentage",
    y = "Density"
  ) +
  theme_report() +
  theme(legend.position = "none")

ggsave("../../007_visualizations/pp_check.png", plot = pp_check_plot, width = 10, height = 6, dpi = 300)

```
