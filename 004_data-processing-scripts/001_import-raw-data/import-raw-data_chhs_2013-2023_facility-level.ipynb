{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "27864e34dd3947848699db4e61b0ea02",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 33,
        "execution_start": 1726508659035,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "5fa57126bc754c8f81ac5c7f988504be",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 34,
        "execution_start": 1726506338984,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Defnining a function to download and combine excel files into a dataframe. \n",
        "# The function also adds a source_file and year column to identify which file a record came from and the year of the data it pertains to.\n",
        "\n",
        "# Takes as input a list of urls, and a number of rows to skip, which varies by year.\n",
        "\n",
        "def download_excel_files(urls, rows_to_skip, sheet_number = 1):\n",
        "    all_data = []\n",
        "\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        # Download an Excel file\n",
        "        response = requests.get(url)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            # Read the Excel file into a BytesIO object\n",
        "            excel_file = BytesIO(response.content)\n",
        "            \n",
        "            # Read the Excel file, specifying dtype for CENSUS_KEY and CENS_TRACT\n",
        "            df = pd.read_excel(\n",
        "                excel_file, \n",
        "                skiprows=rows_to_skip, \n",
        "                sheet_name=sheet_number,\n",
        "                dtype={'CENSUS_KEY': str, 'CENS_TRACT': str, 'FAC_NO': str, 'OSHPD_ID': str}\n",
        "            )\n",
        "            \n",
        "            # Rename 'CENSUS_KEY' to 'CENS_TRACT' if it exists\n",
        "            if 'CENSUS_KEY' in df.columns:\n",
        "                df = df.rename(columns={'CENSUS_KEY': 'CENS_TRACT'})\n",
        "            \n",
        "            # Extract year from the URL\n",
        "            year_match = re.search(r'/spcl(\\d{2})', url)\n",
        "            if year_match:\n",
        "                year = int(year_match.group(1))\n",
        "                full_year = 2000 + year if year < 50 else 1900 + year\n",
        "            else:\n",
        "                full_year = None\n",
        "            \n",
        "            # Add columns to identify which file this data came from\n",
        "            df['source_file'] = f\"file_{i}\"\n",
        "            df['year'] = full_year\n",
        "            \n",
        "            all_data.append(df)\n",
        "            \n",
        "            print(f\"Downloaded and processed file {i}\")\n",
        "        else:\n",
        "            print(f\"Failed to download file from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    if all_data:\n",
        "        # Combine all DataFrames into one\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data was successfully downloaded.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "13e4ca1eadba4bc6a25aa16bfd433345",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 48,
        "execution_start": 1726506338985,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Attempted to use beautifulsoup to scrape the specialty care clinic file urls, but neither xpath or css selector worked reliably. \n",
        "\n",
        "# I split the urls into 2 lists, pre_2018_urls and post_2018_urls, on account of inconsistent data structure.\n",
        "\n",
        "# I also define a data dictionary url to create a dataframe that can be used to standardize column names between the newer and older datasets. \n",
        "\n",
        "# The plan is to create two dataframes for pre- and post-2018, modify the former's structure to match the latter, and combine.\n",
        "\n",
        "# Define a list of urls for files pertaining to 2013–2017\n",
        "\n",
        "pre_2018_urls = [\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/896c699c-07fc-4049-bda0-ff98ac8e3913/download/spcl13utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/91fa31b7-8f40-47f1-8bca-bbc063221993/download/spcl14utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/171f7631-4cb2-4b20-b238-d5ab3512ae10/download/spcl15utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/c6a99713-427a-44df-947d-d46c3402a4d6/download/spcl16_util_data_final-ver2.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e7a2def1-c0dd-41af-a283-46e095bc0af2/download/spcl17_util_data_final.xlsx\"\n",
        "]\n",
        "\n",
        "# Define a list of urls for files pertaining to 2018–2023\n",
        "\n",
        "post_2018_urls = [\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/8ad9b464-cbbd-4ad5-b37d-d2daa924768b/download/spcl23_util_data_prelim.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/00a9d637-d75a-4ba5-9ed5-87bb01f3a6e3/download/spcl22_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/f6339c46-8e35-4466-b972-ce132c43cbf4/download/spcl21_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/9c883633-b661-4da3-b39f-50536f60e573/download/spcl20_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/188b31e3-2307-479e-9bee-632083f902ba/download/spcl19_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e891cdff-6092-4316-b406-dcbcf4a9c016/download/spcl18_util_data_final.xlsx\"\n",
        "]\n",
        "\n",
        "data_dictionary_url = [\"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/188b31e3-2307-479e-9bee-632083f902ba/download/spcl19_util_data_final.xlsx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "a5428c1e93d74f038f9c71cbd69238b1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 10820,
        "execution_start": 1726506339031,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and processed file 1\n",
            "Downloaded and processed file 2\n",
            "Downloaded and processed file 3\n",
            "Downloaded and processed file 4\n",
            "Downloaded and processed file 5\n",
            "Downloaded and processed file 1\n",
            "Downloaded and processed file 2\n",
            "Downloaded and processed file 3\n",
            "Downloaded and processed file 4\n",
            "Downloaded and processed file 5\n",
            "Downloaded and processed file 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Iris.Lin\\AppData\\Local\\Temp\\ipykernel_22416\\1483052699.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat(all_data, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and processed file 1\n"
          ]
        }
      ],
      "source": [
        "pre_2018_df = download_excel_files(pre_2018_urls, rows_to_skip = [1,2,3])\n",
        "\n",
        "post_2018_df = download_excel_files(post_2018_urls, rows_to_skip = [1,2,3,4])\n",
        "\n",
        "data_dictionary = download_excel_files(data_dictionary_url, rows_to_skip = 0, sheet_number = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6d3dfb745fc6459095a11e6b69b6ac0b",
        "deepnote_cell_type": "text-cell-h1",
        "formattedRanges": []
      },
      "source": [
        "# Data cleaning to merge the two sets of historical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "ccfab77fef254791bb9a36f0af6440cc",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 416,
        "execution_start": 1726506350251,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary of old an new column names to rename columns in the pre-2018 dataframe.\n",
        "\n",
        "old_names = data_dictionary[\"ALIRTS Dataset Header (2017)\"]\n",
        "\n",
        "new_names = data_dictionary[\"SIERA Dataset Header (2019)\"]\n",
        "\n",
        "name_mapping = dict(zip(old_names, new_names))\n",
        "\n",
        "# Renaming the columns in the pre-2018 dataframe.\n",
        "\n",
        "pre_2018_df = pre_2018_df.rename(columns=name_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "f25ba0f20a504ab3b68f36041e6a4157",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 41,
        "execution_start": 1726506778363,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to combine street address columns in pre-2018 dataframe.\n",
        "\n",
        "def combine_street_address(df, col1, col2, new_col_name):\n",
        "    \n",
        "    # Combine columns, handling NaN values\n",
        "    df[new_col_name] = df[col1].fillna('').astype(str) + df[col2].fillna('').apply(lambda x: f', {x}' if x else '')\n",
        "    \n",
        "    # Remove trailing comma and space if col2 was empty\n",
        "    df[new_col_name] = df[new_col_name].str.rstrip(', ')\n",
        "\n",
        "    # Remove original columns\n",
        "    df.drop(columns=[col1, col2], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "10dd65f831d446d38d463f94abe480df",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 262,
        "execution_start": 1726506963055,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating combined street adress and parent company address columns in the pre-2018 dataframe.\n",
        "\n",
        "combine_street_address(pre_2018_df, \"FAC_ADDRESS_ONE\", \"FAC_ADDRESS_TWO\", \"FAC_STR_ADDR\")\n",
        "\n",
        "combine_street_address(pre_2018_df, \"PARENT_ADDRESS_ONE\", \"PARENT_ADDRESS_TWO\", \"FAC_PAR_CORP_BUS_ADDR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "0d86f1815a644e0b8629defc627ebcc1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1726507102848,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns only in df1: {'ACLAIMS_NO', 'LIC_STATUS_DATE', 'LIC_ORIG_DATE', 'REPORT_STATUS', 'MCAL_PROVIDER_NO', 'MCARE_PROVIDER_NO'}\n",
            "Columns only in df2: {'HCAI_PROJ_NO_02', 'LICENSE_EFF_DATE', 'CORRECTED_DT', 'LICENSE_EXP_DATE', 'HCAI_PROJ_NO_05', 'HCAI_PROJ_NO_04', 'FACILITY_LEVEL', 'SUBMITTED_DT', 'REV_REPT_PREP_NAME', 'HCAI_PROJ_NO_01', 'HCAI_PROJ_NO_03', 'REVISED_DT', 'Description'}\n"
          ]
        }
      ],
      "source": [
        "# Creating a function to compare columns in the pre-2018 and post-2018 dataframes.\n",
        "# Possibly useful for data validation/future merging steps.\n",
        "\n",
        "def compare_columns(df1, df2):\n",
        "    set1 = set(df1.columns)\n",
        "    set2 = set(df2.columns)\n",
        "    \n",
        "    only_in_df1 = set1 - set2\n",
        "    only_in_df2 = set2 - set1\n",
        "    \n",
        "    return only_in_df1, only_in_df2\n",
        "\n",
        "columns_only_in_df1, columns_only_in_df2 = compare_columns(pre_2018_df, post_2018_df)\n",
        "\n",
        "print(\"Columns only in df1:\", columns_only_in_df1)\n",
        "print(\"Columns only in df2:\", columns_only_in_df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "f341df9a707b48398bb01b5bf9d8f2bd",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 19,
        "execution_start": 1726507324195,
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "706"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_2018_df[\"FAC_NO\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "0eeecc3181fd4070995805311f18643c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 46,
        "execution_start": 1726507332875,
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "778"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "post_2018_df[\"FAC_NO\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cell_id": "62899e0e51ea49fcbc980329fb373881",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1726507432291,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique values only in df1: {'306394019', '306304494', '306196093', '306240036', '306384038', '306197971', '306374088', '306014330', '306434079', '306304528', '306374538', '306154016', '306013683', '306364061', '306014298', '306564062', '306481027', '306540082', '306494082', '306491037', '306334015', '306544074', '306304142', '306334106', '306364361', '306196511', '306197103', '306244032', '306304573', '306154202', '306014248', '306334440', '306561887', '306564219', '306191165', '306194664', '306154003', '306364172', '306374162', '306234041', '306234021', '306134019', '306190632', '306414059', '306100068', '306364209', '306374179', '306190988', '306014219', '306196503', '306434187', '306304135', '306364125', '306314010', '306105079', '306374517', '306344117', '306191070', '306194574'}\n",
            "Unique values only in df2: {'306330742', '306190599', '306370374', '306190690', '306390560', '306270526', '306340635', '306370620', '306370642', '306380578', '306190450', '306440766', '306500551', '306190473', '306190540', '306280432', '306070424', '306370637', '306190646', '306500694', '306070616', '306410476', '306190407', '306550495', '306300486', '306190420', '306360511', '306430408', '306190510', '306310400', '306300457', '306190350', '306010568', '306390527', '306360481', '306370409', '306190483', '306370391', '306190539', '306364595', '306300375', '306330418', '306010634', '306190756', '306300434', '306330385', '306190748', '306190550', '306040431', '306190648', '306100692', '306360488', '306198403', '306300399', '306390403', '306191350', '306570528', '306190522', '306190449', '306190523', '306190689', '306190693', '306198223', '306390778', '306330516', '306300430', '306560525', '306190730', '306070020', '306190708', '306360585', '306370487', '306494139', '306150427', '306190613', '306340451', '306340471', '306170563', '306100429', '306410676', '306010617', '306300364', '306190482', '306190524', '306100660', '306300565', '306190558', '306190419', '306010583', '306370665', '306410630', '306190464', '306150421', '306190521', '306150453', '306190549', '306330402', '306300718', '306300554', '306150480', '306150452', '306300475', '306430447', '306100490', '306010731', '306190553', '306190677', '306330509', '306190493', '306350682', '306190393', '306340405', '306420679', '306300433', '306190618', '306190949', '306380466', '306240684', '306560448', '306330437', '306190631', '306100467', '306190489', '306190723', '306540435', '306430520', '306540615', '306370519', '306340683', '306100647', '306190465'}\n",
            "Unique values in both: {'306304533', '306304474', '306073651', '306574012', '306194037', '306196087', '306074124', '306484050', '306441904', '306212807', '306214066', '306370973', '306420512', '306244056', '306197037', '306374279', '306197120', '306196431', '306196346', '306364367', '306194091', '306274095', '306371591', '306191167', '306344201', '306374075', '306334625', '306344171', '306197868', '306014274', '306301675', '306190819', '306334534', '306374413', '306301270', '306364240', '306014221', '306196266', '306074057', '306197100', '306154200', '306484068', '306502390', '306214046', '306104002', '306394122', '306344121', '306384020', '306154169', '306154125', '306105135', '306384235', '306197428', '306197915', '306196215', '306014294', '306374313', '306344186', '306014344', '306197843', '306190913', '306196610', '306197722', '306564124', '306564157', '306196545', '306544093', '306334667', '306204029', '306564068', '306434288', '306364528', '306196737', '306364567', '306194684', '306330002', '306394108', '306105102', '306544078', '306196918', '306334523', '306196746', '306564145', '306204030', '306434251', '306434182', '306564235', '306384034', '306191424', '306360055', '306364521', '306197264', '306105129', '306312304', '306544002', '306074073', '306304607', '306197663', '306014291', '306197554', '306197066', '306105073', '306134023', '306304416', '306374509', '306434208', '306431887', '306364391', '306282294', '306344124', '306314034', '306196526', '306197272', '306334009', '306334648', '306414077', '306331863', '306014127', '306196385', '306434145', '306414132', '306197821', '306196335', '306374425', '306434267', '306434257', '306414065', '306134026', '306304231', '306014318', '306374285', '306196741', '306214065', '306504096', '306196544', '306334651', '306197119', '306374207', '306044024', '306304617', '306334490', '306194998', '306394050', '306344014', '306154158', '306334581', '306564003', '306394078', '306271856', '306334519', '306304450', '306196888', '306198061', '306014004', '306214064', '306364576', '306334732', '306344236', '306014213', '306484031', '306434249', '306074108', '306105076', '306014265', '306074150', '306190442', '306196635', '306424001', '306196014', '306304560', '306434211', '306197036', '306194024', '306191218', '306154138', '306154155', '306334003', '306196430', '306364531', '306364375', '306304317', '306394080', '306342343', '306198065', '306191101', '306334652', '306197537', '306394096', '306304436', '306434213', '306194101', '306154083', '306434286', '306134002', '306244030', '306196512', '306196095', '306197185', '306197189', '306304093', '306197836', '306492308', '306013662', '306197721', '306574019', '306196255', '306304546', '306412747', '306196724', '306304523', '306334560', '306504089', '306244055', '306014009', '306198064', '306194976', '306334501', '306124035', '306014313', '306344143', '306014069', '306404063', '306191299', '306154174', '306304083', '306042207', '306194262', '306334631', '306198091', '306384218', '306334737', '306198493', '306484043', '306154210', '306204025', '306195005', '306504085', '306194610', '306074081', '306344030', '306194707', '306484049', '306194990', '306196889', '306504051', '306304355', '306334002', '306484055', '306334634', '306584006', '306554022', '306344230', '306304323', '306434265', '306196518', '306197819', '306154182', '306434214', '306196787', '306504084', '306244060', '306374506', '306094027', '306197548', '306197011', '306304023', '306197662', '306494053', '306504041', '306244053', '306544100', '306196337', '306196392', '306044157', '306197009', '306194706', '306164033', '306196206', '306364569', '306191387', '306434207', '306494088', '306191416', '306434209', '306014136', '306434271', '306197950', '306196546', '306244059', '306374053', '306191144', '306304432', '306414113', '306191073', '306194003', '306314052', '306424082', '306304562', '306196256', '306414144', '306044161', '306234030', '306364415', '306196040', '306300163', '306198488', '306191443', '306394076', '306454049', '306494069', '306105031', '306374560', '306105078', '306196299', '306196683', '306364122', '306198011', '306194225', '306304266', '306374171', '306196233', '306191087', '306424042', '306314036', '306384237', '306544094', '306301701', '306197561', '306190096', '306344197', '306494123', '306074162', '306194257', '306374245', '306384018', '306271888', '306564295', '306374242', '306304140', '306191012', '306196786', '306198373', '306074101', '306434191', '306434201', '306194171', '306544086', '306344158', '306105108', '306101074', '306431890', '306334449', '306364345', '306198243', '306304624', '306344160', '306014329', '306364100', '306074182', '306196138', '306374303', '306196560', '306434260', '306374005', '306304549', '306198372', '306191166', '306370661', '306197162', '306374248', '306196339', '306504055', '306197522', '306451269', '306121019', '306344043', '306014306', '306191220', '306196434', '306013655', '306344138', '306196146', '306174011', '306197427', '306564293', '306364401', '306304199', '306504092', '306364406', '306194989', '306344165', '306344045', '306384217', '306364396', '306374325', '306374139', '306196891', '306334726', '306394087', '306197112', '306172293', '306197833', '306364448', '306304614', '306196564', '306074106', '306342275', '306196917', '306196890', '306191298', '306384216', nan, '306304354', '306344142', '306364241', '306374208', '306197590', '306434305', '306197006', '306344123', '306374244', '306360029', '306194857', '306494103', '306014263', '306194172', '306344179', '306196116', '306304303', '306196050', '306196758', '306414118', '306300232', '306444027', '306380788', '306434225', '306196717', '306304147', '306364308', '306014155', '306394010', '306414109', '306494102', '306344200', '306394051', '306194545', '306484070', '306391605', '306197878', '306197916', '306300133', '306414110', '306304267', '306105047', '306374393', '306198086', '306424085', '306394088', '306190916', '306074082', '306244052', '306198291', '306300209', '306344136', '306374121', '306194098', '306550065', '306197401', '306360083', '306196566', '306414119', '306105050', '306154179', '306374016', '306191163', '306374326', '306190961', '306544088', '306544085', '306414117', '306370934', '306504037', '306304304', '306314044', '306334575', '306364592', '306134031', '306274054', '306384230', '306360016', '306561168', '306074180', '306364258', '306304418', '306374011', '306564106', '306194321', '306334485', '306154181', '306194274', '306194979', '306374385', '306564094', '306197323', '306304553', '306434274', '306334742', '306304305', '306504077', '306074147', '306197948', '306364573', '306484065', '306304572', '306014083', '306431853', '306514042', '306197988', '306334104', '306196207', '306304507', '306494138', '306191444', '306364392', '306564250', '306304366', '306564182', '306196969', '306194862', '306484042', '306197612', '306195059', '306196219', '306434254', '306504045', '306196584', '306334540', '306197928', '306196282', '306274086', '306196517', '306196565', '306504048', '306304463', '306014279', '306014335', '306434097', '306154209', '306014135', '306364124', '306074200', '306196041', '306191050', '306344148', '306196122', '306304188', '306364023', '306334482', '306374039', '306414122', '306196432', '306194263', '306504031', '306196062', '306394062', '306196699', '306564010', '306196225', '306194961', '306494098', '306404037', '306190912', '306198087', '306198465', '306484057', '306154189', '306197645', '306524014', '306304581', '306414163', '306191261', '306334623', '306434268', '306191181', '306191400', '306197010', '306334613', '306314020', '306281034', '306434303', '306196136', '306364500', '306194082', '306494124', '306294022', '306194833', '306431040', '306105080', '306484063', '306014197', '306544082', '306514035', '306434167', '306504088', '306234039', '306014267', '306300227', '306194959', '306564150', '306300213', '306164030', '306196044', '306194299', '306154015', '306196684', '306154097', '306014215', '306490918', '306164016', '306374305', '306194210', '306364314', '306334578', '306404058', '306104015', '306364077', '306434308', '306420181', '306394089', '306374328', '306364208', '306196092', '306394124', '306160074', '306484052', '306196945', '306360900', '306334656', '306197666', '306584011', '306197958', '306074122', '306197943', '306197962', '306504065', '306364198', '306334126', '306074058'}\n"
          ]
        }
      ],
      "source": [
        "# Comparing unique values in the FAC_NO column between the pre-2018 and post-2018 dataframes.\n",
        "\n",
        "def compare_unique_values(df1, df2, column_name):\n",
        "    # Get unique values from each DataFrame\n",
        "    unique_df1 = set(df1[column_name].unique())\n",
        "    unique_df2 = set(df2[column_name].unique())\n",
        "    \n",
        "    # Find values in df1 but not in df2\n",
        "    only_in_df1 = unique_df1 - unique_df2\n",
        "    \n",
        "    # Find values in df2 but not in df1\n",
        "    only_in_df2 = unique_df2 - unique_df1\n",
        "    \n",
        "    # Find values in both\n",
        "    in_both = unique_df1.intersection(unique_df2)\n",
        "    \n",
        "    return only_in_df1, only_in_df2, in_both\n",
        "\n",
        "column_to_compare = 'FAC_NO'\n",
        "only_in_df1, only_in_df2, in_both = compare_unique_values(pre_2018_df, post_2018_df, column_to_compare)\n",
        "\n",
        "print(f\"Unique values only in df1: {only_in_df1}\")\n",
        "print(f\"Unique values only in df2: {only_in_df2}\")\n",
        "print(f\"Unique values in both: {in_both}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "cell_id": "791a26a738ba4e859cf35ce7cc8f9c18",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 103,
        "execution_start": 1726508366280,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shared column type comparison:\n",
            "                                    df1_type df2_type  match\n",
            "TOT_PSYCHOLOGY_ENCOUNTERS            float64  float64   True\n",
            "EQUIP_VAL_07                         float64  float64   True\n",
            "HEALTH_SVC_AREA                       object   object   True\n",
            "LONGITUDE                            float64  float64   True\n",
            "LIC_CAT                               object   object   True\n",
            "...                                      ...      ...    ...\n",
            "MENTAL_HEALTH_COUNSELING_ENCOUNTERS  float64  float64   True\n",
            "DEEQUIP_05                           float64  float64   True\n",
            "FAC_PAR_CORP_BUS_ADDR                 object   object   True\n",
            "FAC_NO                                object   object   True\n",
            "TOT_ENCOUNTERS                       float64  float64   True\n",
            "\n",
            "[123 rows x 3 columns]\n",
            "\n",
            "Columns with mismatched types:\n",
            "                         df1_type        df2_type  match\n",
            "SENATE_DIST               float64          object  False\n",
            "DEEQUIP_01                float64          object  False\n",
            "CONGRESS_DIST             float64          object  False\n",
            "MEANS_FOR_ACQUISITION_01  float64          object  False\n",
            "DT_ACQUIRE_01             float64  datetime64[ns]  False\n",
            "ASSEMBLY_DIST             float64          object  False\n"
          ]
        }
      ],
      "source": [
        "# Creating a function to compare data types in the shared columns between the pre-2018 and post-2018 dataframes.\n",
        "# Makes it easier to standardize column types (following two cells) before merging pre- and post-2018 dataframes.\n",
        "\n",
        "def compare_shared_column_types(df1, df2):\n",
        "    # Find shared columns\n",
        "    shared_columns = list(set(df1.columns) & set(df2.columns))\n",
        "    \n",
        "    if not shared_columns:\n",
        "        print(\"No shared columns found between the DataFrames\")\n",
        "        return None\n",
        "    \n",
        "    # Compare data types\n",
        "    comparison = {}\n",
        "    for col in shared_columns:\n",
        "        type1 = df1[col].dtype\n",
        "        type2 = df2[col].dtype\n",
        "        comparison[col] = {\n",
        "            'df1_type': type1,\n",
        "            'df2_type': type2,\n",
        "            'match': type1 == type2\n",
        "        }\n",
        "    \n",
        "    # Converting to DataFrame for easy viewing\n",
        "    comparison_df = pd.DataFrame.from_dict(comparison, orient='index')\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "type_comparison = compare_shared_column_types(pre_2018_df, post_2018_df)\n",
        "\n",
        "if type_comparison is not None:\n",
        "    print(\"Shared column type comparison:\")\n",
        "    print(type_comparison)\n",
        "    \n",
        "    # Identify mismatched columns\n",
        "    mismatched = type_comparison[type_comparison['match'] == False]\n",
        "    if not mismatched.empty:\n",
        "        print(\"\\nColumns with mismatched types:\")\n",
        "        print(mismatched)\n",
        "    else:\n",
        "        print(\"\\nAll shared columns have matching types.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cell_id": "0b45828dd9c844bb89e260330c90abba",
        "deepnote_cell_type": "code",
        "deepnote_table_loading": false,
        "deepnote_table_state": {
          "conditionalFilters": [
            {
              "column": "MEANS_FOR_ACQUISITION_01",
              "comparativeValues": [],
              "operator": "is-not-null"
            }
          ],
          "filters": [],
          "pageIndex": 0,
          "pageSize": 10,
          "sortBy": []
        },
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 254,
        "execution_start": 1726509278543,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Converting aquisition-related columns in pre-2018 dataframe to types in post-2018 dataframe.\n",
        "\n",
        "# Pre-2018 dataframe doesn't contain any values for these columns pre-2018, which is why they were imputed differently from those post-2018, which do contain information.\n",
        "\n",
        "pre_2018_df[\"DT_ACQUIRE_01\"] = pd.to_datetime(pre_2018_df[\"DT_ACQUIRE_01\"])\n",
        "\n",
        "pre_2018_df[\"DEEQUIP_01\"] = pre_2018_df[\"DEEQUIP_01\"].astype(\"str\")\n",
        "\n",
        "pre_2018_df[\"MEANS_FOR_ACQUISITION_01\"] = pre_2018_df[\"MEANS_FOR_ACQUISITION_01\"].astype(\"str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cell_id": "133712a9940349dfac48a8a063457e6c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 141,
        "execution_start": 1726508780347,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to remove the string \"District \" from SENATE_DIST, ASSEMBLY_DIST, and CONGRESS_DIST columns in post-2018 dataframe. \n",
        "\n",
        "def clean_and_convert_to_numeric(df, columns):\n",
        "    def clean_numeric(value):\n",
        "        if pd.isna(value):\n",
        "            return value\n",
        "        # Remove all non-digit characters\n",
        "        cleaned = re.sub(r'\\D', '', str(value))\n",
        "        return cleaned if cleaned else None\n",
        "\n",
        "    for col in columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        # Apply the cleaning function and convert to integer\n",
        "        df[col] = df[col].apply(clean_numeric).astype('Int64')\n",
        "    \n",
        "    return df\n",
        "\n",
        "columns_to_clean = ['SENATE_DIST', 'CONGRESS_DIST', 'ASSEMBLY_DIST']\n",
        "post_2018_df = clean_and_convert_to_numeric(post_2018_df, columns_to_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "cell_id": "e2546f2850824c71bdae003e4a2c8975",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 52,
        "execution_start": 1726508258160,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to merge the pre-2018 and post-2018 dataframes.\n",
        "# Finds all shared columns between df1 and df2 using set intersection.\n",
        "# Checks if there are any shared columns. If not, it raises an error.\n",
        "# Merges the DataFrames using all shared columns.\n",
        "\n",
        "# Using how='outer' ensures that all rows from both DataFrames are kept, even if there's no match on all shared columns.\n",
        "# suffixes=('_df1', '_df2') are added to disambiguate column names that are in both DataFrames but weren't used for merging.\n",
        "\n",
        "def merge_on_shared_columns(df1, df2):\n",
        "    # Find shared columns\n",
        "    shared_columns = list(set(df1.columns) & set(df2.columns))\n",
        "    \n",
        "    # Ensure there are shared columns\n",
        "    if not shared_columns:\n",
        "        raise ValueError(\"No shared columns found between the DataFrames\")\n",
        "    \n",
        "    # Merge DataFrames on all shared columns\n",
        "    merged_df = pd.merge(df1, df2, on=shared_columns, how='outer', suffixes=('_df1', '_df2'))\n",
        "    \n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cell_id": "ac0879287b8647d4a3fbda0be552ad04",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 340,
        "execution_start": 1726508261067,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merge successful\n",
            "Shape of merged DataFrame: (7281, 142)\n"
          ]
        }
      ],
      "source": [
        "# Attempting to merge pre-2018 and post-2018 dataframes using shared columns\n",
        "# If successful, print the shape of the merged dataframe\n",
        "# If unsuccessful due to no shared columns, catch and print the error\n",
        "\n",
        "try:\n",
        "    merged_df = merge_on_shared_columns(pre_2018_df, post_2018_df)\n",
        "    print(\"Merge successful\")\n",
        "    print(f\"Shape of merged DataFrame: {merged_df.shape}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of FAC_NO only in pre_2018_df: 59\n",
            "Number of FAC_NO only in post_2018_df: 131\n",
            "Number of FAC_NO in both dataframes: 647\n",
            "Total unique FAC_NO across both dataframes: 837\n"
          ]
        }
      ],
      "source": [
        "def compare_fac_no(pre_2018_df, post_2018_df):\n",
        "    # Get unique FAC_NO values from each dataframe\n",
        "    pre_2018_fac_no = set(pre_2018_df['FAC_NO'].dropna().unique())\n",
        "    post_2018_fac_no = set(post_2018_df['FAC_NO'].dropna().unique())\n",
        "\n",
        "    # Find FAC_NO values only in pre_2018_df\n",
        "    only_in_pre = pre_2018_fac_no - post_2018_fac_no\n",
        "\n",
        "    # Find FAC_NO values only in post_2018_df\n",
        "    only_in_post = post_2018_fac_no - pre_2018_fac_no\n",
        "\n",
        "    # Find FAC_NO values in both dataframes\n",
        "    in_both = pre_2018_fac_no.intersection(post_2018_fac_no)\n",
        "\n",
        "    print(f\"Number of FAC_NO only in pre_2018_df: {len(only_in_pre)}\")\n",
        "    print(f\"Number of FAC_NO only in post_2018_df: {len(only_in_post)}\")\n",
        "    print(f\"Number of FAC_NO in both dataframes: {len(in_both)}\")\n",
        "    print(f\"Total unique FAC_NO across both dataframes: {len(pre_2018_fac_no.union(post_2018_fac_no))}\")\n",
        "\n",
        "# Use the function\n",
        "compare_fac_no(pre_2018_df, post_2018_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "cell_id": "1fa8bbbbcde346ad81084320e5eba3ac",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 156,
        "execution_start": 1726507915365,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# The only na values in the FAC_NO column are from metadata in the pre-2018 dataframe.\n",
        "# Dropping these rows so all our rows are actual observations.\n",
        "\n",
        "merged_df = merged_df.dropna(subset=['FAC_NO'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to ../../003_data/001_raw-data/2013-2023_CHHS_dialysis-facility_data.parquet\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Converting some columns to string to match the data types in the post-2018 dataframe\n",
        "# Necessary because the data types are inconsistent across the two dataframes\n",
        "def convert_problematic_columns(df):\n",
        "    for col in df.columns:\n",
        "        # Check if column contains any non-numeric values\n",
        "        if df[col].dtype == 'object' and not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # Convert to string, replacing NaN with an empty string\n",
        "            df[col] = df[col].fillna('').astype(str)\n",
        "        elif df[col].dtype == 'object' and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # If it's all numeric, convert to float (which can handle NaN)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# Apply the conversion function\n",
        "merged_df = convert_problematic_columns(merged_df)\n",
        "\n",
        "# Generating a timestamp for the filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Defining output path without timestamp to replace the prior version\n",
        "output_path = '../../003_data/001_raw-data/2013-2023_CHHS_dialysis-facility_data.parquet'\n",
        "\n",
        "# Saving the merged dataframe as a parquet file\n",
        "# Using parquet in order to preserve data types, optimize storage, and improve read performance.\n",
        "\n",
        "try:\n",
        "    # Saving as Parquet\n",
        "    # Using compression='snappy' to optimize storage\n",
        "    merged_df.to_parquet(output_path, index=False, compression='snappy')\n",
        "\n",
        "    # Add metadata\n",
        "    table = pa.Table.from_pandas(merged_df)\n",
        "    metadata = table.schema.metadata\n",
        "\n",
        "    metadata.update({\n",
        "        b'created_at': str(datetime.now()).encode('utf-8'),\n",
        "        b'description': b'Merged specialty care data',\n",
        "        b'version': b'1.0',\n",
        "        b'cleaning_steps': b'''\n",
        "            1. Standardized naming convention and data types for Census Tract and Facility Number columns.\n",
        "            2. Created columns during import: \n",
        "                a. year column to identify the year of the data it pertains to.\n",
        "                b. source_file column to identify the original file from which the data was obtained.\n",
        "            3. Renamed columns in the pre-2018 dataframe to match the post-2018 dataframe using a data dictionary.\n",
        "            4. Combined street address columns in the pre-2018 dataframe.\n",
        "            5. Cleaned and converted specific columns to numeric types in the post-2018 dataframe.\n",
        "            6. Converted acquisition-related columns in the pre-2018 dataframe to match the data types in the post-2018 dataframe.\n",
        "            7. Dropped rows with missing FAC_NO in the merged dataframe.\n",
        "            8. Converted columns with mixed types to string and numeric columns to float in the merged dataframe.\n",
        "        '''\n",
        "    })\n",
        "\n",
        "    updated_table = table.replace_schema_metadata(metadata)\n",
        "    pq.write_table(updated_table, output_path)\n",
        "\n",
        "    print(f\"Data saved to {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving data: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "32da149562af4ca6b675835936c28be0",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
