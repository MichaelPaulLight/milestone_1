{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "cell_id": "27864e34dd3947848699db4e61b0ea02",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 33,
        "execution_start": 1726508659035,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "cell_id": "5fa57126bc754c8f81ac5c7f988504be",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 34,
        "execution_start": 1726506338984,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Defnining a function to download and combine excel files into a dataframe. \n",
        "# The function also adds a source_file and year column to identify which file a record came from and the year of the data it pertains to.\n",
        "\n",
        "# Takes as input a list of urls, and a number of rows to skip, which varies by year.\n",
        "\n",
        "def download_excel_files(urls, rows_to_skip, sheet_number = 1):\n",
        "    all_data = []\n",
        "\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        # Download an Excel file\n",
        "        response = requests.get(url)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            # Read the Excel file into a BytesIO object\n",
        "            excel_file = BytesIO(response.content)\n",
        "            \n",
        "            # Read the Excel file, specifying dtype for CENSUS_KEY and CENS_TRACT\n",
        "            df = pd.read_excel(\n",
        "                excel_file, \n",
        "                skiprows=rows_to_skip, \n",
        "                sheet_name=sheet_number,\n",
        "                dtype={'CENSUS_KEY': str, 'CENS_TRACT': str, 'FAC_NO': str, 'OSHPD_ID': str}\n",
        "            )\n",
        "            \n",
        "            # Rename 'CENSUS_KEY' to 'CENS_TRACT' if it exists\n",
        "            if 'CENSUS_KEY' in df.columns:\n",
        "                df = df.rename(columns={'CENSUS_KEY': 'CENS_TRACT'})\n",
        "            \n",
        "            # Extract year from the URL\n",
        "            year_match = re.search(r'/spcl(\\d{2})', url)\n",
        "            if year_match:\n",
        "                year = int(year_match.group(1))\n",
        "                full_year = 2000 + year if year < 50 else 1900 + year\n",
        "            else:\n",
        "                full_year = None\n",
        "            \n",
        "            # Add columns to identify which file this data came from\n",
        "            df['source_file'] = f\"file_{i}\"\n",
        "            df['year'] = full_year\n",
        "            \n",
        "            all_data.append(df)\n",
        "            \n",
        "            #print(f\"Downloaded and processed file {i}\")\n",
        "        else:\n",
        "            print(f\"Failed to download file from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    if all_data:\n",
        "        # Combine all DataFrames into one\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data was successfully downloaded.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "cell_id": "13e4ca1eadba4bc6a25aa16bfd433345",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 48,
        "execution_start": 1726506338985,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Attempted to use beautifulsoup to scrape the specialty care clinic file urls, but neither xpath or css selector worked reliably. \n",
        "# I split the urls into 2 lists, pre_2018_urls and post_2018_urls, on account of inconsistent data structure.\n",
        "# I also define a data dictionary url to create a dataframe that can be used to standardize column names between the newer and older datasets. \n",
        "# The plan is to create two dataframes for pre- and post-2018, modify the former's structure to match the latter, and combine.\n",
        "# Define a list of urls for files pertaining to 2013–2017\n",
        "\n",
        "pre_2018_urls = [\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/896c699c-07fc-4049-bda0-ff98ac8e3913/download/spcl13utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/91fa31b7-8f40-47f1-8bca-bbc063221993/download/spcl14utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/171f7631-4cb2-4b20-b238-d5ab3512ae10/download/spcl15utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/c6a99713-427a-44df-947d-d46c3402a4d6/download/spcl16_util_data_final-ver2.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e7a2def1-c0dd-41af-a283-46e095bc0af2/download/spcl17_util_data_final.xlsx\"\n",
        "]\n",
        "\n",
        "# Define a list of urls for files pertaining to 2018–2023\n",
        "\n",
        "post_2018_urls = [\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/8ad9b464-cbbd-4ad5-b37d-d2daa924768b/download/spcl23_util_data_prelim.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/00a9d637-d75a-4ba5-9ed5-87bb01f3a6e3/download/spcl22_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/f6339c46-8e35-4466-b972-ce132c43cbf4/download/spcl21_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/9c883633-b661-4da3-b39f-50536f60e573/download/spcl20_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/188b31e3-2307-479e-9bee-632083f902ba/download/spcl19_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e891cdff-6092-4316-b406-dcbcf4a9c016/download/spcl18_util_data_final.xlsx\"\n",
        "]\n",
        "\n",
        "data_dictionary_url = [\"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/188b31e3-2307-479e-9bee-632083f902ba/download/spcl19_util_data_final.xlsx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "a5428c1e93d74f038f9c71cbd69238b1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 10820,
        "execution_start": 1726506339031,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# running download excel file on pre and post 2018 url dicts\n",
        "pre_2018_df = download_excel_files(pre_2018_urls, rows_to_skip = [1,2,3])\n",
        "\n",
        "post_2018_df = download_excel_files(post_2018_urls, rows_to_skip = [1,2,3,4])\n",
        "\n",
        "data_dictionary = download_excel_files(data_dictionary_url, rows_to_skip = 0, sheet_number = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6d3dfb745fc6459095a11e6b69b6ac0b",
        "deepnote_cell_type": "text-cell-h1",
        "formattedRanges": []
      },
      "source": [
        "# Data cleaning to merge the two sets of historical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cell_id": "ccfab77fef254791bb9a36f0af6440cc",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 416,
        "execution_start": 1726506350251,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary of old an new column names to rename columns in the pre-2018 dataframe.\n",
        "\n",
        "old_names = data_dictionary[\"ALIRTS Dataset Header (2017)\"]\n",
        "\n",
        "new_names = data_dictionary[\"SIERA Dataset Header (2019)\"]\n",
        "\n",
        "name_mapping = dict(zip(old_names, new_names))\n",
        "\n",
        "# Renaming the columns in the pre-2018 dataframe.\n",
        "\n",
        "pre_2018_df = pre_2018_df.rename(columns=name_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cell_id": "f25ba0f20a504ab3b68f36041e6a4157",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 41,
        "execution_start": 1726506778363,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to combine street address columns in pre-2018 dataframe.\n",
        "\n",
        "def combine_street_address(df, col1, col2, new_col_name):\n",
        "    \n",
        "    # Combine columns, handling NaN values\n",
        "    df[new_col_name] = df[col1].fillna('').astype(str) + df[col2].fillna('').apply(lambda x: f', {x}' if x else '')\n",
        "    \n",
        "    # Remove trailing comma and space if col2 was empty\n",
        "    df[new_col_name] = df[new_col_name].str.rstrip(', ')\n",
        "\n",
        "    # Remove original columns\n",
        "    df.drop(columns=[col1, col2], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "10dd65f831d446d38d463f94abe480df",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 262,
        "execution_start": 1726506963055,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating combined street adress and parent company address columns in the pre-2018 dataframe.\n",
        "\n",
        "combine_street_address(pre_2018_df, \"FAC_ADDRESS_ONE\", \"FAC_ADDRESS_TWO\", \"FAC_STR_ADDR\").head()\n",
        "\n",
        "combine_street_address(pre_2018_df, \"PARENT_ADDRESS_ONE\", \"PARENT_ADDRESS_TWO\", \"FAC_PAR_CORP_BUS_ADDR\").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0d86f1815a644e0b8629defc627ebcc1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1726507102848,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to compare columns in the pre-2018 and post-2018 dataframes.\n",
        "# Possibly useful for data validation/future merging steps.\n",
        "\n",
        "def compare_columns(df1, df2):\n",
        "    set1 = set(df1.columns)\n",
        "    set2 = set(df2.columns)\n",
        "    \n",
        "    only_in_df1 = set1 - set2\n",
        "    only_in_df2 = set2 - set1\n",
        "    \n",
        "    return only_in_df1, only_in_df2\n",
        "\n",
        "columns_only_in_df1, columns_only_in_df2 = compare_columns(pre_2018_df, post_2018_df)\n",
        "\n",
        "print(\"Columns only in df1:\", columns_only_in_df1)\n",
        "print(\"Columns only in df2:\", columns_only_in_df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f341df9a707b48398bb01b5bf9d8f2bd",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 19,
        "execution_start": 1726507324195,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "pre_2018_df[\"FAC_NO\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0eeecc3181fd4070995805311f18643c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 46,
        "execution_start": 1726507332875,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "post_2018_df[\"FAC_NO\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "62899e0e51ea49fcbc980329fb373881",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1726507432291,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Comparing unique values in the FAC_NO column between the pre-2018 and post-2018 dataframes.\n",
        "\n",
        "def compare_unique_values(df1, df2, column_name):\n",
        "    # Get unique values from each DataFrame\n",
        "    unique_df1 = set(df1[column_name].unique())\n",
        "    unique_df2 = set(df2[column_name].unique())\n",
        "    \n",
        "    # Find values in df1 but not in df2\n",
        "    only_in_df1 = unique_df1 - unique_df2\n",
        "    \n",
        "    # Find values in df2 but not in df1\n",
        "    only_in_df2 = unique_df2 - unique_df1\n",
        "    \n",
        "    # Find values in both\n",
        "    in_both = unique_df1.intersection(unique_df2)\n",
        "    \n",
        "    return only_in_df1, only_in_df2, in_both\n",
        "\n",
        "column_to_compare = 'FAC_NO'\n",
        "only_in_df1, only_in_df2, in_both = compare_unique_values(pre_2018_df, post_2018_df, column_to_compare)\n",
        "\n",
        "#print(f\"Unique values only in df1: {only_in_df1}\")\n",
        "#print(f\"Unique values only in df2: {only_in_df2}\")\n",
        "#print(f\"Unique values in both: {in_both}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "791a26a738ba4e859cf35ce7cc8f9c18",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 103,
        "execution_start": 1726508366280,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to compare data types in the shared columns between the pre-2018 and post-2018 dataframes.\n",
        "# Makes it easier to standardize column types (following two cells) before merging pre- and post-2018 dataframes.\n",
        "\n",
        "def compare_shared_column_types(df1, df2):\n",
        "    # Find shared columns\n",
        "    shared_columns = list(set(df1.columns) & set(df2.columns))\n",
        "    \n",
        "    if not shared_columns:\n",
        "        print(\"No shared columns found between the DataFrames\")\n",
        "        return None\n",
        "    \n",
        "    # Compare data types\n",
        "    comparison = {}\n",
        "    for col in shared_columns:\n",
        "        type1 = df1[col].dtype\n",
        "        type2 = df2[col].dtype\n",
        "        comparison[col] = {\n",
        "            'df1_type': type1,\n",
        "            'df2_type': type2,\n",
        "            'match': type1 == type2\n",
        "        }\n",
        "    \n",
        "    # Converting to DataFrame for easy viewing\n",
        "    comparison_df = pd.DataFrame.from_dict(comparison, orient='index')\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "type_comparison = compare_shared_column_types(pre_2018_df, post_2018_df)\n",
        "\n",
        "if type_comparison is not None:\n",
        "    print(\"Shared column type comparison:\")\n",
        "    print(type_comparison)\n",
        "    \n",
        "    # Identify mismatched columns\n",
        "    mismatched = type_comparison[type_comparison['match'] == False]\n",
        "    if not mismatched.empty:\n",
        "        print(\"\\nColumns with mismatched types:\")\n",
        "        print(mismatched)\n",
        "    else:\n",
        "        print(\"\\nAll shared columns have matching types.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "cell_id": "0b45828dd9c844bb89e260330c90abba",
        "deepnote_cell_type": "code",
        "deepnote_table_loading": false,
        "deepnote_table_state": {
          "conditionalFilters": [
            {
              "column": "MEANS_FOR_ACQUISITION_01",
              "comparativeValues": [],
              "operator": "is-not-null"
            }
          ],
          "filters": [],
          "pageIndex": 0,
          "pageSize": 10,
          "sortBy": []
        },
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 254,
        "execution_start": 1726509278543,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Converting aquisition-related columns in pre-2018 dataframe to types in post-2018 dataframe.\n",
        "\n",
        "# Pre-2018 dataframe doesn't contain any values for these columns pre-2018, which is why they were imputed differently from those post-2018, which do contain information.\n",
        "\n",
        "pre_2018_df[\"DT_ACQUIRE_01\"] = pd.to_datetime(pre_2018_df[\"DT_ACQUIRE_01\"])\n",
        "\n",
        "pre_2018_df[\"DEEQUIP_01\"] = pre_2018_df[\"DEEQUIP_01\"].astype(\"str\")\n",
        "\n",
        "pre_2018_df[\"MEANS_FOR_ACQUISITION_01\"] = pre_2018_df[\"MEANS_FOR_ACQUISITION_01\"].astype(\"str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cell_id": "133712a9940349dfac48a8a063457e6c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 141,
        "execution_start": 1726508780347,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to remove the string \"District \" from SENATE_DIST, ASSEMBLY_DIST, and CONGRESS_DIST columns in post-2018 dataframe. \n",
        "\n",
        "def clean_and_convert_to_numeric(df, columns):\n",
        "    def clean_numeric(value):\n",
        "        if pd.isna(value):\n",
        "            return value\n",
        "        # Remove all non-digit characters\n",
        "        cleaned = re.sub(r'\\D', '', str(value))\n",
        "        return cleaned if cleaned else None\n",
        "\n",
        "    for col in columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        # Apply the cleaning function and convert to integer\n",
        "        df[col] = df[col].apply(clean_numeric).astype('Int64')\n",
        "    \n",
        "    return df\n",
        "\n",
        "columns_to_clean = ['SENATE_DIST', 'CONGRESS_DIST', 'ASSEMBLY_DIST']\n",
        "post_2018_df = clean_and_convert_to_numeric(post_2018_df, columns_to_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cell_id": "e2546f2850824c71bdae003e4a2c8975",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 52,
        "execution_start": 1726508258160,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to merge the pre-2018 and post-2018 dataframes.\n",
        "# Finds all shared columns between df1 and df2 using set intersection.\n",
        "# Checks if there are any shared columns. If not, it raises an error.\n",
        "# Merges the DataFrames using all shared columns.\n",
        "\n",
        "# Using how='outer' ensures that all rows from both DataFrames are kept, even if there's no match on all shared columns.\n",
        "# suffixes=('_df1', '_df2') are added to disambiguate column names that are in both DataFrames but weren't used for merging.\n",
        "\n",
        "def merge_on_shared_columns(df1, df2):\n",
        "    # Find shared columns\n",
        "    shared_columns = list(set(df1.columns) & set(df2.columns))\n",
        "    \n",
        "    # Ensure there are shared columns\n",
        "    if not shared_columns:\n",
        "        raise ValueError(\"No shared columns found between the DataFrames\")\n",
        "    \n",
        "    # Merge DataFrames on all shared columns\n",
        "    merged_df = pd.merge(df1, df2, on=shared_columns, how='outer', suffixes=('_df1', '_df2'))\n",
        "    \n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "ac0879287b8647d4a3fbda0be552ad04",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 340,
        "execution_start": 1726508261067,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Attempting to merge pre-2018 and post-2018 dataframes using shared columns\n",
        "# If successful, print the shape of the merged dataframe\n",
        "# If unsuccessful due to no shared columns, catch and print the error\n",
        "\n",
        "try:\n",
        "    merged_df = merge_on_shared_columns(pre_2018_df, post_2018_df)\n",
        "    print(\"Merge successful\")\n",
        "    print(f\"Shape of merged DataFrame: {merged_df.shape}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_fac_no(pre_2018_df, post_2018_df):\n",
        "    # Get unique FAC_NO values from each dataframe\n",
        "    pre_2018_fac_no = set(pre_2018_df['FAC_NO'].dropna().unique())\n",
        "    post_2018_fac_no = set(post_2018_df['FAC_NO'].dropna().unique())\n",
        "\n",
        "    # Find FAC_NO values only in pre_2018_df\n",
        "    only_in_pre = pre_2018_fac_no - post_2018_fac_no\n",
        "\n",
        "    # Find FAC_NO values only in post_2018_df\n",
        "    only_in_post = post_2018_fac_no - pre_2018_fac_no\n",
        "\n",
        "    # Find FAC_NO values in both dataframes\n",
        "    in_both = pre_2018_fac_no.intersection(post_2018_fac_no)\n",
        "\n",
        "    print(f\"Number of FAC_NO only in pre_2018_df: {len(only_in_pre)}\")\n",
        "    print(f\"Number of FAC_NO only in post_2018_df: {len(only_in_post)}\")\n",
        "    print(f\"Number of FAC_NO in both dataframes: {len(in_both)}\")\n",
        "    print(f\"Total unique FAC_NO across both dataframes: {len(pre_2018_fac_no.union(post_2018_fac_no))}\")\n",
        "\n",
        "# Use the function\n",
        "compare_fac_no(pre_2018_df, post_2018_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "cell_id": "1fa8bbbbcde346ad81084320e5eba3ac",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 156,
        "execution_start": 1726507915365,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# The only na values in the FAC_NO column are from metadata in the pre-2018 dataframe.\n",
        "# Dropping these rows so all our rows are actual observations.\n",
        "\n",
        "merged_df = merged_df.dropna(subset=['FAC_NO'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Converting some columns to string to match the data types in the post-2018 dataframe\n",
        "# Necessary because the data types are inconsistent across the two dataframes\n",
        "def convert_problematic_columns(df):\n",
        "    for col in df.columns:\n",
        "        # Check if column contains any non-numeric values\n",
        "        if df[col].dtype == 'object' and not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # Convert to string, replacing NaN with an empty string\n",
        "            df[col] = df[col].fillna('').astype(str)\n",
        "        elif df[col].dtype == 'object' and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # If it's all numeric, convert to float (which can handle NaN)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# Apply the conversion function\n",
        "merged_df = convert_problematic_columns(merged_df)\n",
        "\n",
        "# Generating a timestamp for the filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Defining output path without timestamp to replace the prior version\n",
        "output_path = '../../003_data/001_raw-data/2013-2023_CHHS_dialysis-facility_data.parquet'\n",
        "\n",
        "# Saving the merged dataframe as a parquet file\n",
        "# Using parquet in order to preserve data types, optimize storage, and improve read performance.\n",
        "\n",
        "try:\n",
        "    # Saving as Parquet\n",
        "    # Using compression='snappy' to optimize storage\n",
        "    merged_df.to_parquet(output_path, index=False, compression='snappy')\n",
        "\n",
        "    # Add metadata\n",
        "    table = pa.Table.from_pandas(merged_df)\n",
        "    metadata = table.schema.metadata\n",
        "\n",
        "    metadata.update({\n",
        "        b'created_at': str(datetime.now()).encode('utf-8'),\n",
        "        b'description': b'Merged specialty care data',\n",
        "        b'version': b'1.0',\n",
        "        b'cleaning_steps': b'''\n",
        "            1. Standardized naming convention and data types for Census Tract and Facility Number columns.\n",
        "            2. Created columns during import: \n",
        "                a. year column to identify the year of the data it pertains to.\n",
        "                b. source_file column to identify the original file from which the data was obtained.\n",
        "            3. Renamed columns in the pre-2018 dataframe to match the post-2018 dataframe using a data dictionary.\n",
        "            4. Combined street address columns in the pre-2018 dataframe.\n",
        "            5. Cleaned and converted specific columns to numeric types in the post-2018 dataframe.\n",
        "            6. Converted acquisition-related columns in the pre-2018 dataframe to match the data types in the post-2018 dataframe.\n",
        "            7. Dropped rows with missing FAC_NO in the merged dataframe.\n",
        "            8. Converted columns with mixed types to string and numeric columns to float in the merged dataframe.\n",
        "        '''\n",
        "    })\n",
        "\n",
        "    updated_table = table.replace_schema_metadata(metadata)\n",
        "    pq.write_table(updated_table, output_path)\n",
        "\n",
        "    print(f\"Data saved to {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving data: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "32da149562af4ca6b675835936c28be0",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
