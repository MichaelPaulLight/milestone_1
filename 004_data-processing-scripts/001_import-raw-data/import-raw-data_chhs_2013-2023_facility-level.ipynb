{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "27864e34dd3947848699db4e61b0ea02",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 33,
        "execution_start": 1726508659035,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import re\n",
        "from datetime import datetime\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cell_id": "5fa57126bc754c8f81ac5c7f988504be",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 34,
        "execution_start": 1726506338984,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Defnining a function to download and combine excel files into a dataframe. \n",
        "# The function also adds a source_file and year column to identify which file a record came from and the year of the data it pertains to.\n",
        "\n",
        "# Takes as input a list of urls, and a number of rows to skip, which varies by year.\n",
        "\n",
        "def download_excel_files(urls, rows_to_skip, sheet_number = 1):\n",
        "    all_data = []\n",
        "\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        # Download an Excel file\n",
        "        response = requests.get(url)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            # Read the Excel file into a BytesIO object\n",
        "            excel_file = BytesIO(response.content)\n",
        "            \n",
        "            # Read the Excel file, specifying dtype for CENSUS_KEY and CENS_TRACT\n",
        "            df = pd.read_excel(\n",
        "                excel_file, \n",
        "                skiprows=rows_to_skip, \n",
        "                sheet_name=sheet_number,\n",
        "                dtype={'CENSUS_KEY': str, 'CENS_TRACT': str, 'FAC_NO': str, 'OSHPD_ID': str}\n",
        "            )\n",
        "            \n",
        "            # Rename 'CENSUS_KEY' to 'CENS_TRACT' if it exists\n",
        "            if 'CENSUS_KEY' in df.columns:\n",
        "                df = df.rename(columns={'CENSUS_KEY': 'CENS_TRACT'})\n",
        "            \n",
        "            # Extract year from the URL\n",
        "            year_match = re.search(r'/spcl(\\d{2})', url)\n",
        "            if year_match:\n",
        "                year = int(year_match.group(1))\n",
        "                full_year = 2000 + year if year < 50 else 1900 + year\n",
        "            else:\n",
        "                full_year = None\n",
        "            \n",
        "            # Add columns to identify which file this data came from\n",
        "            df['source_file'] = f\"file_{i}\"\n",
        "            df['year'] = full_year\n",
        "            \n",
        "            all_data.append(df)\n",
        "            \n",
        "            print(f\"Downloaded and processed file {i}\")\n",
        "        else:\n",
        "            print(f\"Failed to download file from {url}. Status code: {response.status_code}\")\n",
        "\n",
        "    if all_data:\n",
        "        # Combine all DataFrames into one\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data was successfully downloaded.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "13e4ca1eadba4bc6a25aa16bfd433345",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 48,
        "execution_start": 1726506338985,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Attempted to use beautifulsoup to scrape the specialty care clinic file urls, but neither xpath or css selector worked reliably. \n",
        "\n",
        "# I split the urls into 2 lists, pre_2018_urls and post_2018_urls, on account of inconsistent data structure.\n",
        "\n",
        "# I also define a data dictionary url to create a dataframe that can be used to standardize column names between the newer and older datasets. \n",
        "\n",
        "# The plan is to create two dataframes for pre- and post-2018, modify the former's structure to match the latter, and combine.\n",
        "\n",
        "# Define a list of urls for files pertaining to 2013–2017\n",
        "\n",
        "pre_2018_urls = [\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/896c699c-07fc-4049-bda0-ff98ac8e3913/download/spcl13utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/91fa31b7-8f40-47f1-8bca-bbc063221993/download/spcl14utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/171f7631-4cb2-4b20-b238-d5ab3512ae10/download/spcl15utildatafinal.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/c6a99713-427a-44df-947d-d46c3402a4d6/download/spcl16_util_data_final-ver2.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e7a2def1-c0dd-41af-a283-46e095bc0af2/download/spcl17_util_data_final.xlsx\"\n",
        "]\n",
        "\n",
        "# Define a list of urls for files pertaining to 2018–2023\n",
        "\n",
        "post_2018_urls = [\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/8ad9b464-cbbd-4ad5-b37d-d2daa924768b/download/spcl23_util_data_prelim.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/00a9d637-d75a-4ba5-9ed5-87bb01f3a6e3/download/spcl22_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/f6339c46-8e35-4466-b972-ce132c43cbf4/download/spcl21_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/9c883633-b661-4da3-b39f-50536f60e573/download/spcl20_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/188b31e3-2307-479e-9bee-632083f902ba/download/spcl19_util_data_final.xlsx\",\n",
        "    \"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e891cdff-6092-4316-b406-dcbcf4a9c016/download/spcl18_util_data_final.xlsx\"\n",
        "]\n",
        "\n",
        "data_dictionary_url = [\"https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/188b31e3-2307-479e-9bee-632083f902ba/download/spcl19_util_data_final.xlsx\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "a5428c1e93d74f038f9c71cbd69238b1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 10820,
        "execution_start": 1726506339031,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and processed file 1\n",
            "Downloaded and processed file 2\n",
            "Downloaded and processed file 3\n",
            "Downloaded and processed file 4\n",
            "Downloaded and processed file 5\n",
            "Downloaded and processed file 1\n",
            "Downloaded and processed file 2\n",
            "Downloaded and processed file 3\n",
            "Downloaded and processed file 4\n",
            "Downloaded and processed file 5\n",
            "Failed to download file from https://data.chhs.ca.gov/dataset/17bbc0b0-869e-4168-b03b-48fa60c78577/resource/e891cdff-6092-4316-b406-dcbcf4a9c016/download/spcl18_util_data_final.xlsx. Status code: 502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/29/ysfdqqh96xd2zvnqnqd_8fm80000gn/T/ipykernel_75556/1483052699.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat(all_data, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded and processed file 1\n"
          ]
        }
      ],
      "source": [
        "pre_2018_df = download_excel_files(pre_2018_urls, rows_to_skip = [1,2,3])\n",
        "\n",
        "post_2018_df = download_excel_files(post_2018_urls, rows_to_skip = [1,2,3,4])\n",
        "\n",
        "data_dictionary = download_excel_files(data_dictionary_url, rows_to_skip = 0, sheet_number = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "6d3dfb745fc6459095a11e6b69b6ac0b",
        "deepnote_cell_type": "text-cell-h1",
        "formattedRanges": []
      },
      "source": [
        "# Data cleaning to merge the two sets of historical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "ccfab77fef254791bb9a36f0af6440cc",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 416,
        "execution_start": 1726506350251,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a dictionary of old an new column names to rename columns in the pre-2018 dataframe.\n",
        "\n",
        "old_names = data_dictionary[\"ALIRTS Dataset Header (2017)\"]\n",
        "\n",
        "new_names = data_dictionary[\"SIERA Dataset Header (2019)\"]\n",
        "\n",
        "name_mapping = dict(zip(old_names, new_names))\n",
        "\n",
        "# Renaming the columns in the pre-2018 dataframe.\n",
        "\n",
        "pre_2018_df = pre_2018_df.rename(columns=name_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cell_id": "f25ba0f20a504ab3b68f36041e6a4157",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 41,
        "execution_start": 1726506778363,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to combine street address columns in pre-2018 dataframe.\n",
        "\n",
        "def combine_street_address(df, col1, col2, new_col_name):\n",
        "    \n",
        "    # Combine columns, handling NaN values\n",
        "    df[new_col_name] = df[col1].fillna('').astype(str) + df[col2].fillna('').apply(lambda x: f', {x}' if x else '')\n",
        "    \n",
        "    # Remove trailing comma and space if col2 was empty\n",
        "    df[new_col_name] = df[new_col_name].str.rstrip(', ')\n",
        "\n",
        "    # Remove original columns\n",
        "    df.drop(columns=[col1, col2], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "10dd65f831d446d38d463f94abe480df",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 262,
        "execution_start": 1726506963055,
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FAC_NO</th>\n",
              "      <th>FAC_NAME</th>\n",
              "      <th>FAC_CITY</th>\n",
              "      <th>FAC_ZIP</th>\n",
              "      <th>FAC_PHONE</th>\n",
              "      <th>FAC_ADMIN_NAME</th>\n",
              "      <th>FAC_OPERATED_THIS_YR</th>\n",
              "      <th>FAC_OP_PER_BEGIN_DT</th>\n",
              "      <th>FAC_OP_PER_END_DT</th>\n",
              "      <th>FAC_PAR_CORP_NAME</th>\n",
              "      <th>...</th>\n",
              "      <th>DEPROJ_04</th>\n",
              "      <th>PROJ_EXPENDITURES_04</th>\n",
              "      <th>OSHPD_PROJ_NO_04</th>\n",
              "      <th>DEPROJ_05</th>\n",
              "      <th>PROJ_EXPENDITURES_05</th>\n",
              "      <th>OSHPD_PROJ_NO_05</th>\n",
              "      <th>source_file</th>\n",
              "      <th>year</th>\n",
              "      <th>FAC_STR_ADDR</th>\n",
              "      <th>FAC_PAR_CORP_BUS_ADDR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>306013662</td>\n",
              "      <td>UNION CITY DIALYSIS CENTER</td>\n",
              "      <td>UNION CITY</td>\n",
              "      <td>94587</td>\n",
              "      <td>253-733-4847</td>\n",
              "      <td>Vicki Kertzman</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2013-12-31</td>\n",
              "      <td>DaVita HealthCare Partners</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_1</td>\n",
              "      <td>2013</td>\n",
              "      <td>32930 ALVARADO NILES ROAD NO.300</td>\n",
              "      <td>1423 Pacific Ave</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>306013683</td>\n",
              "      <td>BERKELEY DIALYSIS</td>\n",
              "      <td>BERKELEY</td>\n",
              "      <td>94705</td>\n",
              "      <td>253-733-4847</td>\n",
              "      <td>Vicki Kertzman</td>\n",
              "      <td>No</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2013-12-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_1</td>\n",
              "      <td>2013</td>\n",
              "      <td>2920 TELEGRAPH AVENUE</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>306104002</td>\n",
              "      <td>ALLIANT INTERNATIONAL UNIVERSITY</td>\n",
              "      <td>FRESNO</td>\n",
              "      <td>93727</td>\n",
              "      <td>559-253-2277</td>\n",
              "      <td>Robert N. Harris, Ph.D.</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2013-12-31</td>\n",
              "      <td>Alliant International University</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_1</td>\n",
              "      <td>2013</td>\n",
              "      <td>5130 EAST CLINTON WAY</td>\n",
              "      <td>5130 East Clinton Way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>306121019</td>\n",
              "      <td>FRESENIUS MEDICAL CARE OF EUREKA</td>\n",
              "      <td>EUREKA</td>\n",
              "      <td>95501</td>\n",
              "      <td>707-445-2033</td>\n",
              "      <td>Clarke Sabandal</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2013-12-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_1</td>\n",
              "      <td>2013</td>\n",
              "      <td>2765 TIMBER RIDGE LANE</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>306134002</td>\n",
              "      <td>EL CENTRO DESERT VALLEY DIALYSIS CENTER</td>\n",
              "      <td>EL CENTRO</td>\n",
              "      <td>92243</td>\n",
              "      <td>760-353-0353</td>\n",
              "      <td>Ana Charves</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2013-01-01</td>\n",
              "      <td>2013-12-31</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_1</td>\n",
              "      <td>2013</td>\n",
              "      <td>110 SO. FIFTH STREET</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3074</th>\n",
              "      <td>306190916</td>\n",
              "      <td>ROWLAND HEIGHTS DIALYSIS</td>\n",
              "      <td>CITY OF INDUSTRY</td>\n",
              "      <td>91748</td>\n",
              "      <td>626-964-5849</td>\n",
              "      <td>Katherine Crespo</td>\n",
              "      <td>No</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017-12-31</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_5</td>\n",
              "      <td>2017</td>\n",
              "      <td>17875 COLIMA RD,  STE A</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3075</th>\n",
              "      <td>306334726</td>\n",
              "      <td>FRESENIUS MEDICAL CARE MORENO VALLEY</td>\n",
              "      <td>MORENO VALLEY</td>\n",
              "      <td>92555</td>\n",
              "      <td>951-242-9196</td>\n",
              "      <td>JASON BAUER</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017-12-31</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_5</td>\n",
              "      <td>2017</td>\n",
              "      <td>27420 IRIS AVE.</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3076</th>\n",
              "      <td>306196095</td>\n",
              "      <td>FMC DIALYSIS SERVICES OF WEST COVINA</td>\n",
              "      <td>WEST COVINA</td>\n",
              "      <td>91790</td>\n",
              "      <td>626-337-8007</td>\n",
              "      <td>ANIL VAIDYA</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017-12-31</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_5</td>\n",
              "      <td>2017</td>\n",
              "      <td>1540 W WEST COVINA PKWY</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3077</th>\n",
              "      <td>306304303</td>\n",
              "      <td>RAI - LAGUNA CANYON - IRVINE</td>\n",
              "      <td>IRVINE</td>\n",
              "      <td>92618-3603</td>\n",
              "      <td>949-727-4495</td>\n",
              "      <td>JASON BAUER</td>\n",
              "      <td>Yes</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>2017-12-31</td>\n",
              "      <td></td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_5</td>\n",
              "      <td>2017</td>\n",
              "      <td>16255 LAGUNA CANYON RD</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3078</th>\n",
              "      <td>NaN</td>\n",
              "      <td>n = 662</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>file_5</td>\n",
              "      <td>2017</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3079 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         FAC_NO                                 FAC_NAME           FAC_CITY  \\\n",
              "0     306013662               UNION CITY DIALYSIS CENTER         UNION CITY   \n",
              "1     306013683                        BERKELEY DIALYSIS           BERKELEY   \n",
              "2     306104002         ALLIANT INTERNATIONAL UNIVERSITY             FRESNO   \n",
              "3     306121019         FRESENIUS MEDICAL CARE OF EUREKA             EUREKA   \n",
              "4     306134002  EL CENTRO DESERT VALLEY DIALYSIS CENTER          EL CENTRO   \n",
              "...         ...                                      ...                ...   \n",
              "3074  306190916                 ROWLAND HEIGHTS DIALYSIS   CITY OF INDUSTRY   \n",
              "3075  306334726     FRESENIUS MEDICAL CARE MORENO VALLEY      MORENO VALLEY   \n",
              "3076  306196095     FMC DIALYSIS SERVICES OF WEST COVINA        WEST COVINA   \n",
              "3077  306304303             RAI - LAGUNA CANYON - IRVINE             IRVINE   \n",
              "3078        NaN                                  n = 662                NaN   \n",
              "\n",
              "          FAC_ZIP     FAC_PHONE           FAC_ADMIN_NAME FAC_OPERATED_THIS_YR  \\\n",
              "0           94587  253-733-4847           Vicki Kertzman                  Yes   \n",
              "1           94705  253-733-4847           Vicki Kertzman                   No   \n",
              "2           93727  559-253-2277  Robert N. Harris, Ph.D.                  Yes   \n",
              "3           95501  707-445-2033          Clarke Sabandal                  Yes   \n",
              "4           92243  760-353-0353              Ana Charves                  Yes   \n",
              "...           ...           ...                      ...                  ...   \n",
              "3074        91748  626-964-5849         Katherine Crespo                   No   \n",
              "3075        92555  951-242-9196              JASON BAUER                  Yes   \n",
              "3076        91790  626-337-8007              ANIL VAIDYA                  Yes   \n",
              "3077   92618-3603  949-727-4495              JASON BAUER                  Yes   \n",
              "3078          NaN           NaN                      NaN                  NaN   \n",
              "\n",
              "     FAC_OP_PER_BEGIN_DT FAC_OP_PER_END_DT                 FAC_PAR_CORP_NAME  \\\n",
              "0             2013-01-01        2013-12-31        DaVita HealthCare Partners   \n",
              "1             2013-01-01        2013-12-31                               NaN   \n",
              "2             2013-01-01        2013-12-31  Alliant International University   \n",
              "3             2013-01-01        2013-12-31                               NaN   \n",
              "4             2013-01-01        2013-12-31                               NaN   \n",
              "...                  ...               ...                               ...   \n",
              "3074          2017-01-01        2017-12-31                                     \n",
              "3075          2017-01-01        2017-12-31                                     \n",
              "3076          2017-01-01        2017-12-31                                     \n",
              "3077          2017-01-01        2017-12-31                                     \n",
              "3078                 NaT               NaT                               NaN   \n",
              "\n",
              "      ... DEPROJ_04 PROJ_EXPENDITURES_04 OSHPD_PROJ_NO_04 DEPROJ_05  \\\n",
              "0     ...       NaN                  NaN              NaN       NaN   \n",
              "1     ...       NaN                  NaN              NaN       NaN   \n",
              "2     ...       NaN                  NaN              NaN       NaN   \n",
              "3     ...       NaN                  NaN              NaN       NaN   \n",
              "4     ...       NaN                  NaN              NaN       NaN   \n",
              "...   ...       ...                  ...              ...       ...   \n",
              "3074  ...       NaN                  NaN              NaN       NaN   \n",
              "3075  ...       NaN                  NaN              NaN       NaN   \n",
              "3076  ...       NaN                  NaN              NaN       NaN   \n",
              "3077  ...       NaN                  NaN              NaN       NaN   \n",
              "3078  ...       NaN                  NaN              NaN       NaN   \n",
              "\n",
              "     PROJ_EXPENDITURES_05 OSHPD_PROJ_NO_05 source_file  year  \\\n",
              "0                     NaN              NaN      file_1  2013   \n",
              "1                     NaN              NaN      file_1  2013   \n",
              "2                     NaN              NaN      file_1  2013   \n",
              "3                     NaN              NaN      file_1  2013   \n",
              "4                     NaN              NaN      file_1  2013   \n",
              "...                   ...              ...         ...   ...   \n",
              "3074                  NaN              NaN      file_5  2017   \n",
              "3075                  NaN              NaN      file_5  2017   \n",
              "3076                  NaN              NaN      file_5  2017   \n",
              "3077                  NaN              NaN      file_5  2017   \n",
              "3078                  NaN              NaN      file_5  2017   \n",
              "\n",
              "                          FAC_STR_ADDR  FAC_PAR_CORP_BUS_ADDR  \n",
              "0     32930 ALVARADO NILES ROAD NO.300       1423 Pacific Ave  \n",
              "1                2920 TELEGRAPH AVENUE                         \n",
              "2                5130 EAST CLINTON WAY  5130 East Clinton Way  \n",
              "3               2765 TIMBER RIDGE LANE                         \n",
              "4                 110 SO. FIFTH STREET                         \n",
              "...                                ...                    ...  \n",
              "3074           17875 COLIMA RD,  STE A                         \n",
              "3075                   27420 IRIS AVE.                         \n",
              "3076           1540 W WEST COVINA PKWY                         \n",
              "3077            16255 LAGUNA CANYON RD                         \n",
              "3078                                                           \n",
              "\n",
              "[3079 rows x 129 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating combined street adress and parent company address columns in the pre-2018 dataframe.\n",
        "\n",
        "combine_street_address(pre_2018_df, \"FAC_ADDRESS_ONE\", \"FAC_ADDRESS_TWO\", \"FAC_STR_ADDR\")\n",
        "\n",
        "combine_street_address(pre_2018_df, \"PARENT_ADDRESS_ONE\", \"PARENT_ADDRESS_TWO\", \"FAC_PAR_CORP_BUS_ADDR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "0d86f1815a644e0b8629defc627ebcc1",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 15,
        "execution_start": 1726507102848,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns only in df1: {'ACLAIMS_NO', 'MCARE_PROVIDER_NO', 'MCAL_PROVIDER_NO', 'LIC_ORIG_DATE', 'LIC_STATUS_DATE', 'REPORT_STATUS'}\n",
            "Columns only in df2: {'Description', 'HCAI_PROJ_NO_05', 'FACILITY_LEVEL', 'LICENSE_EXP_DATE', 'HCAI_PROJ_NO_02', 'SUBMITTED_DT', 'REV_REPT_PREP_NAME', 'HCAI_PROJ_NO_03', 'LICENSE_EFF_DATE', 'REVISED_DT', 'CORRECTED_DT', 'HCAI_PROJ_NO_04', 'HCAI_PROJ_NO_01'}\n"
          ]
        }
      ],
      "source": [
        "# Creating a function to compare columns in the pre-2018 and post-2018 dataframes.\n",
        "# Possibly useful for data validation/future merging steps.\n",
        "\n",
        "def compare_columns(df1, df2):\n",
        "    set1 = set(df1.columns)\n",
        "    set2 = set(df2.columns)\n",
        "    \n",
        "    only_in_df1 = set1 - set2\n",
        "    only_in_df2 = set2 - set1\n",
        "    \n",
        "    return only_in_df1, only_in_df2\n",
        "\n",
        "columns_only_in_df1, columns_only_in_df2 = compare_columns(pre_2018_df, post_2018_df)\n",
        "\n",
        "print(\"Columns only in df1:\", columns_only_in_df1)\n",
        "print(\"Columns only in df2:\", columns_only_in_df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cell_id": "f341df9a707b48398bb01b5bf9d8f2bd",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 19,
        "execution_start": 1726507324195,
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "706"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_2018_df[\"FAC_NO\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "0eeecc3181fd4070995805311f18643c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 46,
        "execution_start": 1726507332875,
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "770"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "post_2018_df[\"FAC_NO\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "62899e0e51ea49fcbc980329fb373881",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 78,
        "execution_start": 1726507432291,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique values only in df1: {'306334440', '306191070', '306300213', '306364061', '306564219', '306105079', '306481027', '306196503', '306100068', '306196511', '306194574', '306494082', '306154016', '306304528', '306164016', '306134019', '306344117', '306014248', '306434187', '306544074', '306014330', '306301270', '306364361', '306414059', '306312304', '306334106', '306014219', '306197971', '306374088', '306304494', '306374179', '306244032', '306314010', '306491037', '306304083', '306197916', '306154003', '306364209', '306561887', '306304135', '306374517', '306364172', '306191163', '306304573', '306564062', '306304142', '306190632', '306374538', '306434079', '306013683', '306394019', '306234041', '306197103', '306240036', '306194664', '306154202', '306190988', '306540082', '306374162', '306014298', '306334015', '306196093', '306364125', '306384038', '306234021', '306191165', '306194003'}\n",
            "Unique values only in df2: {'306330516', '306190350', '306380466', '306190693', '306390403', '306330509', '306190464', '306190631', '306370519', '306330742', '306300554', '306190550', '306198403', '306300565', '306100429', '306070424', '306240684', '306300433', '306370637', '306300457', '306190540', '306340451', '306190449', '306540615', '306190523', '306570528', '306330402', '306150452', '306340683', '306300475', '306550495', '306190558', '306190618', '306190646', '306430408', '306370642', '306070616', '306370487', '306190522', '306190510', '306190748', '306430520', '306010583', '306300434', '306190599', '306190539', '306350682', '306010731', '306190613', '306190465', '306100490', '306390560', '306150480', '306190419', '306150421', '306190708', '306360511', '306340405', '306198223', '306330437', '306270526', '306540435', '306560525', '306010617', '306170563', '306190723', '306330418', '306190756', '306100467', '306370665', '306150453', '306190407', '306100660', '306300486', '306010634', '306190489', '306360488', '306360481', '306494139', '306440766', '306364595', '306070020', '306190420', '306410630', '306190482', '306300364', '306430447', '306190730', '306191350', '306190677', '306190690', '306370409', '306190521', '306500694', '306410476', '306340635', '306300430', '306390527', '306190393', '306410676', '306190483', '306310400', '306330385', '306370620', '306300375', '306560448', '306370391', '306190493', '306190648', '306190549', '306300399', '306420679', '306190473', '306190450', '306100692', '306010568', '306360585', '306100647', '306190524', '306380578', '306390778', '306150427', '306190949', '306280432', '306190689', '306500551', '306300718', '306040431', '306190553', '306370374', '306340471'}\n",
            "Unique values in both: {'306196339', '306014263', '306384237', '306197928', '306394088', '306196684', '306334002', '306190819', '306494124', '306344143', '306504037', '306073651', '306434254', '306334575', '306196566', '306244059', '306484049', '306244053', '306105050', '306194610', '306197264', nan, '306544088', '306194037', '306334485', '306394087', '306101074', '306364569', '306544093', '306196969', '306196737', '306504096', '306042207', '306504048', '306394051', '306374560', '306564150', '306431890', '306374244', '306434251', '306197612', '306494103', '306304614', '306196945', '306360900', '306504045', '306374053', '306074106', '306197009', '306105078', '306191012', '306244055', '306196544', '306196219', '306301701', '306304450', '306344030', '306164030', '306196337', '306274054', '306196584', '306304354', '306196699', '306196206', '306414109', '306564106', '306374313', '306194210', '306191261', '306334009', '306370973', '306344123', '306198064', '306154200', '306154209', '306484057', '306304581', '306197948', '306196092', '306191101', '306196758', '306304546', '306194990', '306194225', '306074108', '306344136', '306196565', '306374011', '306334523', '306304523', '306134031', '306394122', '306370934', '306364392', '306014083', '306074147', '306334501', '306374121', '306014306', '306330002', '306344121', '306334490', '306434303', '306384218', '306190912', '306304317', '306196335', '306384216', '306414119', '306197666', '306197010', '306564003', '306105102', '306194545', '306434257', '306198243', '306374139', '306196546', '306194262', '306584011', '306414122', '306374005', '306194024', '306196610', '306494123', '306304432', '306197119', '306484043', '306191444', '306196146', '306514035', '306334631', '306191144', '306344165', '306334104', '306394089', '306196430', '306494138', '306434211', '306364100', '306504077', '306197162', '306502390', '306154158', '306196635', '306434225', '306404037', '306014127', '306191073', '306234039', '306214064', '306196225', '306104015', '306304231', '306374248', '306074057', '306304305', '306334652', '306484068', '306014009', '306414117', '306197537', '306444027', '306300209', '306374279', '306360016', '306434213', '306374075', '306384018', '306504065', '306197112', '306274086', '306504085', '306344124', '306504092', '306282294', '306197036', '306451269', '306434265', '306197878', '306344230', '306154155', '306564250', '306271888', '306196717', '306198493', '306196122', '306334449', '306194989', '306374328', '306504051', '306434305', '306420181', '306194321', '306584006', '306484042', '306197833', '306424001', '306364592', '306334560', '306304023', '306434167', '306374509', '306544082', '306196044', '306484055', '306484031', '306404063', '306334732', '306014267', '306364124', '306196434', '306434182', '306384034', '306244056', '306196233', '306434288', '306196891', '306434208', '306197100', '306304199', '306434191', '306371591', '306334737', '306194707', '306196746', '306074182', '306014155', '306374393', '306160074', '306196087', '306197401', '306196889', '306304140', '306196040', '306544100', '306564295', '306194299', '306384217', '306154138', '306194274', '306314020', '306494102', '306434209', '306334656', '306360029', '306014213', '306134026', '306194857', '306134002', '306404058', '306197943', '306304507', '306194172', '306300163', '306304266', '306191220', '306014294', '306196545', '306074101', '306564010', '306544094', '306154125', '306191416', '306314052', '306504041', '306196786', '306197843', '306550065', '306191424', '306134023', '306494088', '306434207', '306190096', '306196014', '306434097', '306197590', '306564182', '306197958', '306197721', '306074200', '306304188', '306414113', '306434268', '306197915', '306196215', '306304533', '306344160', '306194684', '306331863', '306342275', '306564145', '306196116', '306374242', '306281034', '306197868', '306194098', '306364198', '306391605', '306334625', '306364345', '306074073', '306431887', '306212807', '306564068', '306300232', '306197120', '306196255', '306196526', '306344201', '306014335', '306196346', '306364573', '306304303', '306334726', '306195059', '306374425', '306334581', '306014069', '306191400', '306504055', '306196050', '306194979', '306364258', '306197819', '306197836', '306304624', '306314036', '306434201', '306194998', '306274095', '306344200', '306414077', '306194706', '306304560', '306191299', '306414132', '306197962', '306484050', '306300133', '306360083', '306304553', '306121019', '306196041', '306384230', '306196266', '306304418', '306434267', '306344148', '306294022', '306564235', '306194833', '306504031', '306334634', '306364500', '306564293', '306196512', '306198065', '306364314', '306014318', '306364367', '306414163', '306197066', '306364208', '306384020', '306014265', '306190442', '306196138', '306044161', '306394080', '306374171', '306434145', '306196890', '306384235', '306434308', '306196256', '306174011', '306105129', '306554022', '306196787', '306105047', '306394076', '306014279', '306374208', '306364077', '306196282', '306304416', '306434286', '306394124', '306197561', '306014313', '306196062', '306374305', '306412747', '306334519', '306344186', '306197548', '306154189', '306074150', '306304093', '306364375', '306304617', '306364241', '306196888', '306197037', '306431853', '306244030', '306334482', '306198086', '306344045', '306198091', '306105031', '306424042', '306154210', '306194101', '306364528', '306544002', '306344197', '306380788', '306196517', '306304304', '306194976', '306074081', '306196724', '306014004', '306195005', '306544085', '306484063', '306204029', '306334534', '306304572', '306374207', '306374413', '306194862', '306564157', '306014274', '306334651', '306374506', '306492308', '306364308', '306191298', '306194263', '306364396', '306564124', '306197722', '306198061', '306364240', '306074162', '306198087', '306154169', '306484065', '306190913', '306014221', '306344138', '306574019', '306364448', '306196431', '306364531', '306344043', '306154083', '306364415', '306364023', '306014197', '306454049', '306105135', '306196918', '306014291', '306191167', '306196741', '306198373', '306094027', '306424082', '306334742', '306304549', '306364576', '306484052', '306198372', '306414065', '306374303', '306197272', '306191218', '306304366', '306198011', '306344158', '306374326', '306364122', '306364567', '306191087', '306214066', '306334126', '306301675', '306014136', '306074124', '306561168', '306014329', '306074122', '306334003', '306244052', '306194091', '306234030', '306204025', '306191050', '306196560', '306194257', '306104002', '306434249', '306074180', '306494053', '306154174', '306154182', '306013655', '306394078', '306154097', '306364391', '306124035', '306194171', '306105108', '306154181', '306191181', '306172293', '306014215', '306105080', '306394062', '306374285', '306154015', '306314034', '306364521', '306154179', '306191387', '306394050', '306342343', '306334613', '306197645', '306484070', '306196564', '306197185', '306504089', '306204030', '306271856', '306344236', '306394096', '306414118', '306014344', '306196095', '306344014', '306197663', '306334578', '306304607', '306197189', '306300227', '306434214', '306544086', '306334540', '306414144', '306304267', '306196385', '306197554', '306434260', '306197821', '306490918', '306196432', '306044157', '306564094', '306197988', '306196299', '306196392', '306197427', '306197006', '306394108', '306374016', '306314044', '306074058', '306304436', '306394010', '306304562', '306434274', '306191443', '306524014', '306194959', '306504088', '306214046', '306504084', '306304323', '306574012', '306374245', '306197011', '306105073', '306013662', '306194082', '306244060', '306197950', '306194961', '306196917', '306334623', '306514042', '306196207', '306105076', '306196518', '306360055', '306344171', '306334648', '306190916', '306414110', '306197428', '306374385', '306370661', '306344179', '306374039', '306304355', '306014135', '306304463', '306304147', '306214065', '306441904', '306334667', '306198465', '306494098', '306198291', '306344142', '306074082', '306364401', '306164033', '306304474', '306197662', '306420512', '306494069', '306197323', '306044024', '306197522', '306424085', '306544078', '306196136', '306191166', '306364406', '306374325', '306431040', '306434271', '306190961', '306196683', '306198488'}\n"
          ]
        }
      ],
      "source": [
        "# Comparing unique values in the FAC_NO column between the pre-2018 and post-2018 dataframes.\n",
        "\n",
        "def compare_unique_values(df1, df2, column_name):\n",
        "    # Get unique values from each DataFrame\n",
        "    unique_df1 = set(df1[column_name].unique())\n",
        "    unique_df2 = set(df2[column_name].unique())\n",
        "    \n",
        "    # Find values in df1 but not in df2\n",
        "    only_in_df1 = unique_df1 - unique_df2\n",
        "    \n",
        "    # Find values in df2 but not in df1\n",
        "    only_in_df2 = unique_df2 - unique_df1\n",
        "    \n",
        "    # Find values in both\n",
        "    in_both = unique_df1.intersection(unique_df2)\n",
        "    \n",
        "    return only_in_df1, only_in_df2, in_both\n",
        "\n",
        "column_to_compare = 'FAC_NO'\n",
        "only_in_df1, only_in_df2, in_both = compare_unique_values(pre_2018_df, post_2018_df, column_to_compare)\n",
        "\n",
        "print(f\"Unique values only in df1: {only_in_df1}\")\n",
        "print(f\"Unique values only in df2: {only_in_df2}\")\n",
        "print(f\"Unique values in both: {in_both}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "791a26a738ba4e859cf35ce7cc8f9c18",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 103,
        "execution_start": 1726508366280,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shared column type comparison:\n",
            "                                 df1_type df2_type  match\n",
            "SUBSTANCE_ABUSE_ENCOUNTERS        float64  float64   True\n",
            "PROJ_EXPENDITURES_01              float64  float64   True\n",
            "EQUIP_VAL_01                      float64  float64   True\n",
            "APP_IN_HOME_TRAINING_CAPD_CCPD     object   object   True\n",
            "OTHER_OPER_REVENUE_DONATIONS_TOT  float64  float64   True\n",
            "...                                   ...      ...    ...\n",
            "SENATE_DIST                       float64   object  False\n",
            "DEEQUIP_05                        float64  float64   True\n",
            "LICENSE_NO                        float64  float64   True\n",
            "FAC_CITY                           object   object   True\n",
            "MEANS_FOR_ACQUISITION_03          float64  float64   True\n",
            "\n",
            "[123 rows x 3 columns]\n",
            "\n",
            "Columns with mismatched types:\n",
            "                         df1_type        df2_type  match\n",
            "CONGRESS_DIST             float64          object  False\n",
            "DEEQUIP_01                float64          object  False\n",
            "MEANS_FOR_ACQUISITION_01  float64          object  False\n",
            "DT_ACQUIRE_01             float64  datetime64[ns]  False\n",
            "ASSEMBLY_DIST             float64          object  False\n",
            "SENATE_DIST               float64          object  False\n"
          ]
        }
      ],
      "source": [
        "# Creating a function to compare data types in the shared columns between the pre-2018 and post-2018 dataframes.\n",
        "# Makes it easier to standardize column types (following two cells) before merging pre- and post-2018 dataframes.\n",
        "\n",
        "def compare_shared_column_types(df1, df2):\n",
        "    # Find shared columns\n",
        "    shared_columns = list(set(df1.columns) & set(df2.columns))\n",
        "    \n",
        "    if not shared_columns:\n",
        "        print(\"No shared columns found between the DataFrames\")\n",
        "        return None\n",
        "    \n",
        "    # Compare data types\n",
        "    comparison = {}\n",
        "    for col in shared_columns:\n",
        "        type1 = df1[col].dtype\n",
        "        type2 = df2[col].dtype\n",
        "        comparison[col] = {\n",
        "            'df1_type': type1,\n",
        "            'df2_type': type2,\n",
        "            'match': type1 == type2\n",
        "        }\n",
        "    \n",
        "    # Converting to DataFrame for easy viewing\n",
        "    comparison_df = pd.DataFrame.from_dict(comparison, orient='index')\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "type_comparison = compare_shared_column_types(pre_2018_df, post_2018_df)\n",
        "\n",
        "if type_comparison is not None:\n",
        "    print(\"Shared column type comparison:\")\n",
        "    print(type_comparison)\n",
        "    \n",
        "    # Identify mismatched columns\n",
        "    mismatched = type_comparison[type_comparison['match'] == False]\n",
        "    if not mismatched.empty:\n",
        "        print(\"\\nColumns with mismatched types:\")\n",
        "        print(mismatched)\n",
        "    else:\n",
        "        print(\"\\nAll shared columns have matching types.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cell_id": "0b45828dd9c844bb89e260330c90abba",
        "deepnote_cell_type": "code",
        "deepnote_table_loading": false,
        "deepnote_table_state": {
          "conditionalFilters": [
            {
              "column": "MEANS_FOR_ACQUISITION_01",
              "comparativeValues": [],
              "operator": "is-not-null"
            }
          ],
          "filters": [],
          "pageIndex": 0,
          "pageSize": 10,
          "sortBy": []
        },
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 254,
        "execution_start": 1726509278543,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Converting aquisition-related columns in pre-2018 dataframe to types in post-2018 dataframe.\n",
        "\n",
        "# Pre-2018 dataframe doesn't contain any values for these columns pre-2018, which is why they were imputed differently from those post-2018, which do contain information.\n",
        "\n",
        "pre_2018_df[\"DT_ACQUIRE_01\"] = pd.to_datetime(pre_2018_df[\"DT_ACQUIRE_01\"])\n",
        "\n",
        "pre_2018_df[\"DEEQUIP_01\"] = pre_2018_df[\"DEEQUIP_01\"].astype(\"str\")\n",
        "\n",
        "pre_2018_df[\"MEANS_FOR_ACQUISITION_01\"] = pre_2018_df[\"MEANS_FOR_ACQUISITION_01\"].astype(\"str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cell_id": "133712a9940349dfac48a8a063457e6c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 141,
        "execution_start": 1726508780347,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to remove the string \"District \" from SENATE_DIST, ASSEMBLY_DIST, and CONGRESS_DIST columns in post-2018 dataframe. \n",
        "\n",
        "def clean_and_convert_to_numeric(df, columns):\n",
        "    def clean_numeric(value):\n",
        "        if pd.isna(value):\n",
        "            return value\n",
        "        # Remove all non-digit characters\n",
        "        cleaned = re.sub(r'\\D', '', str(value))\n",
        "        return cleaned if cleaned else None\n",
        "\n",
        "    for col in columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Warning: Column '{col}' not found in the DataFrame. Skipping.\")\n",
        "            continue\n",
        "        \n",
        "        # Apply the cleaning function and convert to integer\n",
        "        df[col] = df[col].apply(clean_numeric).astype('Int64')\n",
        "    \n",
        "    return df\n",
        "\n",
        "columns_to_clean = ['SENATE_DIST', 'CONGRESS_DIST', 'ASSEMBLY_DIST']\n",
        "post_2018_df = clean_and_convert_to_numeric(post_2018_df, columns_to_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "e2546f2850824c71bdae003e4a2c8975",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 52,
        "execution_start": 1726508258160,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Creating a function to merge the pre-2018 and post-2018 dataframes.\n",
        "# Finds all shared columns between df1 and df2 using set intersection.\n",
        "# Checks if there are any shared columns. If not, it raises an error.\n",
        "# Merges the DataFrames using all shared columns.\n",
        "\n",
        "# Using how='outer' ensures that all rows from both DataFrames are kept, even if there's no match on all shared columns.\n",
        "# suffixes=('_df1', '_df2') are added to disambiguate column names that are in both DataFrames but weren't used for merging.\n",
        "\n",
        "def merge_on_shared_columns(df1, df2):\n",
        "    # Find shared columns\n",
        "    shared_columns = list(set(df1.columns) & set(df2.columns))\n",
        "    \n",
        "    # Ensure there are shared columns\n",
        "    if not shared_columns:\n",
        "        raise ValueError(\"No shared columns found between the DataFrames\")\n",
        "    \n",
        "    # Merge DataFrames on all shared columns\n",
        "    merged_df = pd.merge(df1, df2, on=shared_columns, how='outer', suffixes=('_df1', '_df2'))\n",
        "    \n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "ac0879287b8647d4a3fbda0be552ad04",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 340,
        "execution_start": 1726508261067,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merge successful\n",
            "Shape of merged DataFrame: (6619, 142)\n"
          ]
        }
      ],
      "source": [
        "# Attempting to merge pre-2018 and post-2018 dataframes using shared columns\n",
        "# If successful, print the shape of the merged dataframe\n",
        "# If unsuccessful due to no shared columns, catch and print the error\n",
        "\n",
        "try:\n",
        "    merged_df = merge_on_shared_columns(pre_2018_df, post_2018_df)\n",
        "    print(\"Merge successful\")\n",
        "    print(f\"Shape of merged DataFrame: {merged_df.shape}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of FAC_NO only in pre_2018_df: 67\n",
            "Number of FAC_NO only in post_2018_df: 131\n",
            "Number of FAC_NO in both dataframes: 639\n",
            "Total unique FAC_NO across both dataframes: 837\n"
          ]
        }
      ],
      "source": [
        "def compare_fac_no(pre_2018_df, post_2018_df):\n",
        "    # Get unique FAC_NO values from each dataframe\n",
        "    pre_2018_fac_no = set(pre_2018_df['FAC_NO'].dropna().unique())\n",
        "    post_2018_fac_no = set(post_2018_df['FAC_NO'].dropna().unique())\n",
        "\n",
        "    # Find FAC_NO values only in pre_2018_df\n",
        "    only_in_pre = pre_2018_fac_no - post_2018_fac_no\n",
        "\n",
        "    # Find FAC_NO values only in post_2018_df\n",
        "    only_in_post = post_2018_fac_no - pre_2018_fac_no\n",
        "\n",
        "    # Find FAC_NO values in both dataframes\n",
        "    in_both = pre_2018_fac_no.intersection(post_2018_fac_no)\n",
        "\n",
        "    print(f\"Number of FAC_NO only in pre_2018_df: {len(only_in_pre)}\")\n",
        "    print(f\"Number of FAC_NO only in post_2018_df: {len(only_in_post)}\")\n",
        "    print(f\"Number of FAC_NO in both dataframes: {len(in_both)}\")\n",
        "    print(f\"Total unique FAC_NO across both dataframes: {len(pre_2018_fac_no.union(post_2018_fac_no))}\")\n",
        "\n",
        "# Use the function\n",
        "compare_fac_no(pre_2018_df, post_2018_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cell_id": "1fa8bbbbcde346ad81084320e5eba3ac",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 156,
        "execution_start": 1726507915365,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# The only na values in the FAC_NO column are from metadata in the pre-2018 dataframe.\n",
        "# Dropping these rows so all our rows are actual observations.\n",
        "\n",
        "merged_df = merged_df.dropna(subset=['FAC_NO'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved to ../../003_data/001_raw-data/2013-2023_CHHS_dialysis-facility_data.parquet\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Converting some columns to string to match the data types in the post-2018 dataframe\n",
        "# Necessary because the data types are inconsistent across the two dataframes\n",
        "def convert_problematic_columns(df):\n",
        "    for col in df.columns:\n",
        "        # Check if column contains any non-numeric values\n",
        "        if df[col].dtype == 'object' and not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # Convert to string, replacing NaN with an empty string\n",
        "            df[col] = df[col].fillna('').astype(str)\n",
        "        elif df[col].dtype == 'object' and pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # If it's all numeric, convert to float (which can handle NaN)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# Apply the conversion function\n",
        "merged_df = convert_problematic_columns(merged_df)\n",
        "\n",
        "# Generating a timestamp for the filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Defining output path without timestamp to replace the prior version\n",
        "output_path = '../../003_data/001_raw-data/2013-2023_CHHS_dialysis-facility_data.parquet'\n",
        "\n",
        "# Saving the merged dataframe as a parquet file\n",
        "# Using parquet in order to preserve data types, optimize storage, and improve read performance.\n",
        "\n",
        "try:\n",
        "    # Saving as Parquet\n",
        "    # Using compression='snappy' to optimize storage\n",
        "    merged_df.to_parquet(output_path, index=False, compression='snappy')\n",
        "\n",
        "    # Add metadata\n",
        "    table = pa.Table.from_pandas(merged_df)\n",
        "    metadata = table.schema.metadata\n",
        "\n",
        "    metadata.update({\n",
        "        b'created_at': str(datetime.now()).encode('utf-8'),\n",
        "        b'description': b'Merged specialty care data',\n",
        "        b'version': b'1.0',\n",
        "        b'cleaning_steps': b'''\n",
        "            1. Standardized naming convention and data types for Census Tract and Facility Number columns.\n",
        "            2. Created columns during import: \n",
        "                a. year column to identify the year of the data it pertains to.\n",
        "                b. source_file column to identify the original file from which the data was obtained.\n",
        "            3. Renamed columns in the pre-2018 dataframe to match the post-2018 dataframe using a data dictionary.\n",
        "            4. Combined street address columns in the pre-2018 dataframe.\n",
        "            5. Cleaned and converted specific columns to numeric types in the post-2018 dataframe.\n",
        "            6. Converted acquisition-related columns in the pre-2018 dataframe to match the data types in the post-2018 dataframe.\n",
        "            7. Dropped rows with missing FAC_NO in the merged dataframe.\n",
        "            8. Converted columns with mixed types to string and numeric columns to float in the merged dataframe.\n",
        "        '''\n",
        "    })\n",
        "\n",
        "    updated_table = table.replace_schema_metadata(metadata)\n",
        "    pq.write_table(updated_table, output_path)\n",
        "\n",
        "    print(f\"Data saved to {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving data: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "32da149562af4ca6b675835936c28be0",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
